{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Importation des package</center></h2>\n",
    "<center>(nombre des lucioles \"nl\", max bound \"MB\",\n",
    "min bound \"mb\",nombre des variables \"nv\",..</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** Importation des packages***************************************************\n",
      "* list des packages: \n",
      "keras,numpy,PIL,SYS,OS,CSV,tensorflow,MATH,OPERATOR,TIME,PSUTIL,matplotlib\n"
     ]
    }
   ],
   "source": [
    "print(\"*************************************************** Importation des packages***************************************************\")\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, BatchNormalization, Activation, Conv2D, MaxPooling2D, Dropout\n",
    "import numpy\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from numpy import expand_dims\n",
    "from tensorflow.python.eager.function import Function\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import time as time\n",
    "import psutil\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"* list des packages: \")\n",
    "print(\"keras,numpy,PIL,SYS,OS,CSV,tensorflow,MATH,OPERATOR,TIME,PSUTIL,matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>initialisation des paramères</center></h2>\n",
    "<center>nombre des lucioles , max bound ,min bound,nombre des variables ,..</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debut.......\n",
      "bound: (array([1., 1.]), array([150., 300.]))\n",
      "****************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"debut.......\")\n",
    "\n",
    "# #DEFINITION OF CNN PARAMETERS\n",
    "batch_size = 5\n",
    "kernel_size = (4,)\n",
    "stride = 1\n",
    "# #EPOCHS AND FILTERS ARE DEFINED BY PARTICLES\n",
    "#\n",
    "#DEFINITION OF firefly PARAMETERS\n",
    "numberFirefly = 6\n",
    "iterations = 4\n",
    "#\n",
    "minBound = numpy.ones(2)  # MIN BOUND FOR TWO DIMENSIONS IS 1\n",
    "maxBound = numpy.ones(2)  # ONLY INITIALIZATION\n",
    "maxBound[0] = 150  # MAX NUMBER OF FILTERS\n",
    "maxBound[1] = 300  # MAX NUMBER OF EPOCHS\n",
    "bounds = (minBound, maxBound)\n",
    "print(\"bound:\",bounds)\n",
    "print(\"****************************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>generation des populations</center></h3>\n",
    "<center><strong> 1/  génération des positions des lucioles </strong> <br>générer  des lucioles aléatoirement \"nl\" firefly chaque firefly contient \"nv\" variables compris entre \"MB\" et \"mb\" </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation des populations\n",
      "population :\n",
      "[[ 68 174]\n",
      " [  3 168]\n",
      " [ 12 104]\n",
      " [ 63  96]\n",
      " [ 39 236]\n",
      " [ 31  33]]\n",
      "*******************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"generation des populations\")\n",
    "from numpy.random import random_sample\n",
    "def generate_population(population_size, problem_dim, min_bound, max_bound):\n",
    "    error = 1e-10\n",
    "    data = (max_bound + error - min_bound) * random_sample((population_size, problem_dim)) + min_bound\n",
    "    #data[data > max_bound] = max_bound\n",
    "    return data\n",
    "\n",
    "fireflies=generate_population(numberFirefly, 2, minBound, maxBound)\n",
    "fireflies = fireflies.astype(int)\n",
    "print(\"population :\")\n",
    "print(fireflies)\n",
    "print(\"*******************************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "export_dataframe_1 = pd.read_excel('export_dataframe.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_foruni=list(export_dataframe_1['Code Fournisseur'].unique())\n",
    "code_num=[i for i in range(len(liste_foruni))]\n",
    "code={}\n",
    "for id,cod in zip(liste_foruni,code_num):\n",
    "    code[id]=cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_fourni(cod):\n",
    "    return code[cod]\n",
    "export_dataframe_1['id_Fournisseur']=export_dataframe_1['Code Fournisseur'].apply(get_id_fourni,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-92c033489a2e>:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  export_dataframe_2=export_dataframe_2.drop(['Ligne', 'Dossier', 'Poids Taxable', 'Catégorie', 'Date présentation',\n"
     ]
    }
   ],
   "source": [
    "export_dataframe_2=export_dataframe_1.set_index(['Date Opération'])\n",
    "export_dataframe_3=export_dataframe_1.set_index(['Date Opération'])\n",
    "export_dataframe_2=export_dataframe_2.drop(['Ligne', 'Dossier', 'Poids Taxable', 'Catégorie', 'Date présentation',\n",
    "       'Devise Prévision', 'Prévision', 'Devise Réalisation','N° Achat', 'Date Achat', 'Code Fournisseur', 'prev_resultat',\n",
    "       'cout_analys'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620084 days of training data \n",
      " 32159 days of testing data \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Réalisation</th>\n",
       "      <th>id_Fournisseur</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date Opération</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>44.571429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>9.523810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>3.904762</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Réalisation  id_Fournisseur\n",
       "Date Opération                             \n",
       "2017-01-02        44.571429               0\n",
       "2017-01-02         9.523810               1\n",
       "2017-01-02         3.904762               2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_date = '2021-12-31'\n",
    "\n",
    "df_training = export_dataframe_2.loc[export_dataframe_2.index <= split_date]\n",
    "df_test = export_dataframe_2.loc[export_dataframe_2.index > split_date]\n",
    "print(f\"{len(df_training)} days of training data \\n {len(df_test)} days of testing data \")\n",
    "\n",
    "#df_training.to_csv('datasets/training.csv')\n",
    "#df_test.to_csv('datasets/test.csv')\n",
    "df_training.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD time features to our model\n",
    "def create_time_features(df, target=None):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df['date'] = df.index\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['sin_day'] = np.sin(df['dayofyear'])\n",
    "    df['cos_day'] = np.cos(df['dayofyear'])\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    X = df.drop(['date'], axis=1)\n",
    "    if target:\n",
    "        y = df[target]\n",
    "        X = X.drop([target], axis=1)\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-3d74ea191bd7>:16: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df['weekofyear'] = df['date'].dt.weekofyear\n",
      "<ipython-input-23-3d74ea191bd7>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = df.index\n",
      "<ipython-input-23-3d74ea191bd7>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hour'] = df['date'].dt.hour\n",
      "<ipython-input-23-3d74ea191bd7>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dayofweek'] = df['date'].dt.dayofweek\n",
      "<ipython-input-23-3d74ea191bd7>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['quarter'] = df['date'].dt.quarter\n",
      "<ipython-input-23-3d74ea191bd7>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['month'] = df['date'].dt.month\n",
      "<ipython-input-23-3d74ea191bd7>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['year'] = df['date'].dt.year\n",
      "<ipython-input-23-3d74ea191bd7>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dayofyear'] = df['date'].dt.dayofyear\n",
      "<ipython-input-23-3d74ea191bd7>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sin_day'] = np.sin(df['dayofyear'])\n",
      "<ipython-input-23-3d74ea191bd7>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cos_day'] = np.cos(df['dayofyear'])\n",
      "<ipython-input-23-3d74ea191bd7>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dayofmonth'] = df['date'].dt.day\n",
      "<ipython-input-23-3d74ea191bd7>:16: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df['weekofyear'] = df['date'].dt.weekofyear\n",
      "<ipython-input-23-3d74ea191bd7>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['weekofyear'] = df['date'].dt.weekofyear\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train_df, y_train = create_time_features(export_dataframe_2, target='Réalisation')\n",
    "X_test_df, y_test = create_time_features(df_test, target='Réalisation')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_df)  # No cheating, never scale on the training+test!\n",
    "X_train = scaler.transform(X_train_df)\n",
    "X_test = scaler.transform(X_test_df)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns=X_train_df.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our dl model we will create windows of data that will be feeded into the datasets, for each timestemp T we will append the data from T-7 to T to the Xdata with target Y(t)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 100\n",
    "WINDOW_LENGTH = 24\n",
    "\n",
    "\n",
    "def window_data(X, Y, window=7):\n",
    "    '''\n",
    "    The dataset length will be reduced to guarante all samples have the window, so new length will be len(dataset)-window\n",
    "    '''\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(window-1, len(X)):\n",
    "        x.append(X[i-window+1:i+1])\n",
    "        y.append(Y[i])\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set equal: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "# Since we are doing sliding, we need to join the datasets again of train and test\n",
    "X_w = np.concatenate((X_train, X_test))\n",
    "y_w = np.concatenate((y_train, y_test))\n",
    "\n",
    "X_w, y_w = window_data(X_w, y_w, window=WINDOW_LENGTH)\n",
    "X_train_w = X_w[:-len(X_test)]\n",
    "y_train_w = y_w[:-len(X_test)]\n",
    "X_test_w = X_w[-len(X_test):]\n",
    "y_test_w = y_w[-len(X_test):]\n",
    "\n",
    "# Check we will have same test set as in the previous models, make sure we didnt screw up on the windowing\n",
    "print(f\"Test set equal: {np.array_equal(y_test_w,y_test)}\")\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train_w, y_train_w))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = tf.data.Dataset.from_tensor_slices((X_test_w, y_test_w))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong> 2/ genétarion des luminisotés des firefly</strong>  <br> initialiser la luminosité 'I' des chaque firefly généré par le taux de perte de modèle basé sur les paramètre de cette firefly</strong></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************declaration de cnn_covid19**********************************************\n",
      "cette fonction va entrainer le modèle crée par les nombre des epochs et filters de chaque firefly et retourner le tauxde perte de cette modèle après le test de cette modèle sur un ensemble de test\n"
     ]
    }
   ],
   "source": [
    "print('*******************************************declaration de cnn_covid19**********************************************')\n",
    "print(\"cette fonction va entrainer le modèle crée par les nombre des epochs et filters de chaque firefly et retourner le taux\"\n",
    "      \"de perte de cette modèle après le test de cette modèle sur un ensemble de test\")\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def Lstm_optm(epochs, filters,save=False):\n",
    "    \n",
    "    dropout = 0.0\n",
    "    simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(\n",
    "        128, input_shape=X_train_w.shape[-2:], dropout=dropout),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    simple_lstm_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "\n",
    "\n",
    "    EVALUATION_INTERVAL = 200\n",
    "    EPOCHS = epochs\n",
    "\n",
    "    model_history = simple_lstm_model.fit(train_data, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data, validation_steps=filters)  # ,callbacks=[tensorboard_callback]) #Uncomment this line for tensorboard support\n",
    "\n",
    "\n",
    "    yhat = simple_lstm_model.predict(X_test_w).reshape(1, -1)[0]\n",
    "    rmse = sqrt(mean_squared_error(y_test, yhat))\n",
    "\n",
    "    return rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************déclaration de optimize CNN****************************************************\n",
      "cette fonction va avoir une luciole contenant le nombre des epochs et des filters généré après on va calculer l'accuracyde modèle basé sur cette nombre des epochs et filters (en passant ces paramètre a la fonction CNN_COVID) et finalement on va calculerle taux de perte en se basant sur l'accuracy et nombre de filter et des epochs\n"
     ]
    }
   ],
   "source": [
    "print(\"******************************************déclaration de optimize CNN****************************************************\")\n",
    "print(\"cette fonction va avoir une luciole contenant le nombre des epochs et des filters généré après on va calculer l'accuracy\"\n",
    "      \"de modèle basé sur cette nombre des epochs et filters (en passant ces paramètre a la fonction CNN_COVID) et finalement on va calculer\" \n",
    "      \"le taux de perte en se basant sur l'accuracy et nombre de filter et des epochs\")\n",
    "def LossCNN(firefly):\n",
    "    try:\n",
    "        \n",
    "        print(firefly)\n",
    "        #RETRIEVE DIMENSIONS OF PARTICLE\n",
    "        numberFilters = int(firefly[0]) #FLOAT TO INT\n",
    "        numberEpochs = int(firefly[1])\n",
    "\n",
    "        #CALL CNN FUNCTION cnn --> RETURN accuracy\n",
    "        rmse = Lstm_optm(epochs=numberEpochs, filters=numberFilters)\n",
    "\n",
    "        #APPLY LOST FUNCTION --> THE MAIN OBJECTIVE IS TO MINIMIZE LOSS --> MAXIMIZE ACCURACY AND AT SAME TIME MINIMIZE THE NUMBER OF EPOCHS\n",
    "                                #AND FILTERS, TO REDUCE TIME AND COMPUTACIONAL POWER\n",
    "        \n",
    "        return rmse\n",
    "\n",
    "    except:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************déclaration au Firefly-Iteration***********************************************\n",
      "cette fonction va boucler sur le nombre des lucioles générer et pour chaque itération on va faire appel a la fonction LossCNN qui va retourne le taux de perte de modèle de firefly passé en paramètre\n"
     ]
    }
   ],
   "source": [
    "print(\"****************************************déclaration au Firefly-Iteration***********************************************\")\n",
    "print(\"cette fonction va boucler sur le nombre des lucioles générer et pour chaque itération on va faire appel \"\n",
    "      \"a la fonction LossCNN qui va retourne le taux de perte de modèle de firefly passé en paramètre\")\n",
    "def FireflyIteration(fireflies):\n",
    "    try:\n",
    "        numberFirefly = fireflies.shape[0]\n",
    "        allLosses = [LossCNN(firefly=fireflies[i])for i in range(numberFirefly)]\n",
    "\n",
    "\n",
    "        return allLosses#NEED TO RETURN THIS PYSWARMS NEED THIS\n",
    "\n",
    "    except:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialisation...............\n",
      "[86 36]\n",
      "Epoch 1/36\n",
      "200/200 [==============================] - 5s 24ms/step - loss: 435724.6250 - val_loss: 425398.8750\n",
      "Epoch 2/36\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 506085.0000 - val_loss: 426183.8125\n",
      "Epoch 3/36\n",
      "200/200 [==============================] - 5s 26ms/step - loss: 585119.1875 - val_loss: 425143.0625\n",
      "Epoch 4/36\n",
      "200/200 [==============================] - 6s 29ms/step - loss: 454336.7500 - val_loss: 427823.2188\n",
      "Epoch 5/36\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 359008.1562 - val_loss: 427992.3125\n",
      "Epoch 6/36\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 527803.4375 - val_loss: 458802.0000\n",
      "Epoch 7/36\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 484538.5938 - val_loss: 436564.8438\n",
      "Epoch 8/36\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 633004.9375 - val_loss: 524923.3750\n",
      "Epoch 9/36\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 838041.1875 - val_loss: 527104.3125\n",
      "Epoch 10/36\n",
      "200/200 [==============================] - 9s 43ms/step - loss: 744028.0000 - val_loss: 487175.8125\n",
      "Epoch 11/36\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 541155.9375 - val_loss: 443291.5000\n",
      "Epoch 12/36\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 636454.0000 - val_loss: 435821.8125\n",
      "Epoch 13/36\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 528095.0625 - val_loss: 431570.7500\n",
      "Epoch 14/36\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 758947.9375 - val_loss: 443272.6875\n",
      "Epoch 15/36\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 450834.7188 - val_loss: 541432.4375\n",
      "Epoch 16/36\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 402245.7500 - val_loss: 520150.0000\n",
      "Epoch 17/36\n",
      "200/200 [==============================] - 10s 48ms/step - loss: 463260.5312 - val_loss: 500310.4062\n",
      "Epoch 18/36\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 424481.8438 - val_loss: 474156.5000\n",
      "Epoch 19/36\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 435427.9062 - val_loss: 481978.1250\n",
      "Epoch 20/36\n",
      "200/200 [==============================] - 11s 57ms/step - loss: 497057.0938 - val_loss: 490976.2812\n",
      "Epoch 21/36\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 406247.8438 - val_loss: 489033.6875\n",
      "Epoch 22/36\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 458295.4375 - val_loss: 476263.9062\n",
      "Epoch 23/36\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 490573.2812 - val_loss: 431933.2188\n",
      "Epoch 24/36\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 428760.6250 - val_loss: 421868.5625\n",
      "Epoch 25/36\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 528354.4375 - val_loss: 431394.7500\n",
      "Epoch 26/36\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 797014.6250 - val_loss: 433824.5938\n",
      "Epoch 27/36\n",
      "200/200 [==============================] - 12s 61ms/step - loss: 372571.2500 - val_loss: 436802.8750\n",
      "Epoch 28/36\n",
      "200/200 [==============================] - 12s 62ms/step - loss: 710012.3750 - val_loss: 440612.0000\n",
      "Epoch 29/36\n",
      "200/200 [==============================] - 12s 61ms/step - loss: 406444.4062 - val_loss: 453789.6875\n",
      "Epoch 30/36\n",
      "200/200 [==============================] - 13s 64ms/step - loss: 3217934.7500 - val_loss: 436555.5312\n",
      "Epoch 31/36\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 526071.7500 - val_loss: 421886.9688\n",
      "Epoch 32/36\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 1335683.6250 - val_loss: 422037.1562\n",
      "Epoch 33/36\n",
      "200/200 [==============================] - 13s 67ms/step - loss: 615337.6875 - val_loss: 422804.1875\n",
      "Epoch 34/36\n",
      "200/200 [==============================] - 13s 67ms/step - loss: 743473.6875 - val_loss: 435146.1875\n",
      "Epoch 35/36\n",
      "200/200 [==============================] - 14s 70ms/step - loss: 795012.2500 - val_loss: 427729.9062\n",
      "Epoch 36/36\n",
      "200/200 [==============================] - 14s 71ms/step - loss: 760751.8125 - val_loss: 425130.5625\n",
      "[65 93]\n",
      "Epoch 1/93\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 435935.1250 - val_loss: 464796.3750\n",
      "Epoch 2/93\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 506452.4375 - val_loss: 465773.8125\n",
      "Epoch 3/93\n",
      "200/200 [==============================] - 8s 38ms/step - loss: 586992.9375 - val_loss: 466396.0938\n",
      "Epoch 4/93\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 450811.3750 - val_loss: 470679.8750\n",
      "Epoch 5/93\n",
      "200/200 [==============================] - 8s 38ms/step - loss: 358560.6250 - val_loss: 481353.1250\n",
      "Epoch 6/93\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 528409.4375 - val_loss: 509594.3438\n",
      "Epoch 7/93\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 484602.2500 - val_loss: 518764.5000\n",
      "Epoch 8/93\n",
      "200/200 [==============================] - 9s 43ms/step - loss: 607177.1875 - val_loss: 556656.1875\n",
      "Epoch 9/93\n",
      "200/200 [==============================] - 9s 45ms/step - loss: 840208.6250 - val_loss: 572609.9375\n",
      "Epoch 10/93\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 767399.2500 - val_loss: 537420.0000\n",
      "Epoch 11/93\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 540728.7500 - val_loss: 465849.0000\n",
      "Epoch 12/93\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 635871.1875 - val_loss: 486847.3125\n",
      "Epoch 13/93\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 528357.5625 - val_loss: 472086.4375\n",
      "Epoch 14/93\n",
      "200/200 [==============================] - 11s 57ms/step - loss: 759596.5000 - val_loss: 466661.4688\n",
      "Epoch 15/93\n",
      "200/200 [==============================] - 10s 50ms/step - loss: 450969.8750 - val_loss: 506567.5625\n",
      "Epoch 16/93\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 404372.6250 - val_loss: 516675.1875\n",
      "Epoch 17/93\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 460012.7500 - val_loss: 519505.6562\n",
      "Epoch 18/93\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 424462.4688 - val_loss: 498219.2500\n",
      "Epoch 19/93\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 436615.6875 - val_loss: 615131.3125\n",
      "Epoch 20/93\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 492127.6250 - val_loss: 585693.2500\n",
      "Epoch 21/93\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 408176.8125 - val_loss: 482560.1875\n",
      "Epoch 22/93\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 456459.1875 - val_loss: 562606.5625\n",
      "Epoch 23/93\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 491943.0000 - val_loss: 466361.5938\n",
      "Epoch 24/93\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 426777.6250 - val_loss: 466618.0312\n",
      "Epoch 25/93\n",
      "200/200 [==============================] - 11s 57ms/step - loss: 527089.9375 - val_loss: 467209.5312\n",
      "Epoch 26/93\n",
      "200/200 [==============================] - 12s 61ms/step - loss: 795848.0000 - val_loss: 470273.6250\n",
      "Epoch 27/93\n",
      "200/200 [==============================] - 12s 62ms/step - loss: 369859.9062 - val_loss: 486274.0000\n",
      "Epoch 28/93\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 708261.9375 - val_loss: 567898.2500\n",
      "Epoch 29/93\n",
      "200/200 [==============================] - 12s 62ms/step - loss: 405360.0312 - val_loss: 606537.5000\n",
      "Epoch 30/93\n",
      "200/200 [==============================] - 13s 63ms/step - loss: 3229149.0000 - val_loss: 558181.9375\n",
      "Epoch 31/93\n",
      "200/200 [==============================] - 13s 66ms/step - loss: 516351.1562 - val_loss: 483310.2500\n",
      "Epoch 32/93\n",
      "200/200 [==============================] - 15s 74ms/step - loss: 1332555.0000 - val_loss: 462652.4062\n",
      "Epoch 33/93\n",
      "200/200 [==============================] - 14s 72ms/step - loss: 616773.0625 - val_loss: 496301.5938\n",
      "Epoch 34/93\n",
      "200/200 [==============================] - 14s 69ms/step - loss: 742695.6250 - val_loss: 493316.7812\n",
      "Epoch 35/93\n",
      "200/200 [==============================] - 15s 76ms/step - loss: 797209.2500 - val_loss: 501697.8438\n",
      "Epoch 36/93\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 768377.0625 - val_loss: 617229.9375\n",
      "Epoch 37/93\n",
      "200/200 [==============================] - 14s 71ms/step - loss: 757718.2500 - val_loss: 550100.3125\n",
      "Epoch 38/93\n",
      "200/200 [==============================] - 13s 67ms/step - loss: 1102270.3750 - val_loss: 489824.7188\n",
      "Epoch 39/93\n",
      "200/200 [==============================] - 14s 72ms/step - loss: 681137.8125 - val_loss: 457717.2500\n",
      "Epoch 40/93\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 514658.0312 - val_loss: 466815.3125\n",
      "Epoch 41/93\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 824085.7500 - val_loss: 464046.2188\n",
      "Epoch 42/93\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 607245.6250 - val_loss: 471728.0000\n",
      "Epoch 43/93\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 1213237.5000 - val_loss: 574890.6250\n",
      "Epoch 44/93\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 3672779.5000 - val_loss: 481214.5625\n",
      "Epoch 45/93\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 43484432.0000 - val_loss: 517075.8125\n",
      "Epoch 46/93\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 3434365.5000 - val_loss: 512211.8750\n",
      "Epoch 47/93\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 1045989.1250 - val_loss: 508020.4375\n",
      "Epoch 48/93\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 700008.5000 - val_loss: 517017.8438\n",
      "Epoch 49/93\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 615665.1250 - val_loss: 455692.5312\n",
      "Epoch 50/93\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 609427.7500 - val_loss: 457944.3125\n",
      "Epoch 51/93\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 479794.3750 - val_loss: 467627.6562\n",
      "Epoch 52/93\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 414235.5938 - val_loss: 459827.2812\n",
      "Epoch 53/93\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 484070.4688 - val_loss: 461103.0000\n",
      "Epoch 54/93\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 556282.6875 - val_loss: 455552.5938\n",
      "Epoch 55/93\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 429563.0938 - val_loss: 459484.6875\n",
      "Epoch 56/93\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 339098.0000 - val_loss: 456421.7812\n",
      "Epoch 57/93\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 479170.8125 - val_loss: 454950.0938\n",
      "Epoch 58/93\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 468016.5938 - val_loss: 467338.2500\n",
      "Epoch 59/93\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 603277.2500 - val_loss: 463238.9688\n",
      "Epoch 60/93\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 806913.9375 - val_loss: 456759.6562\n",
      "Epoch 61/93\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 718304.8750 - val_loss: 463809.9375\n",
      "Epoch 62/93\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 666536.2500 - val_loss: 455924.9688\n",
      "Epoch 63/93\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 471238.3125 - val_loss: 452447.9375\n",
      "Epoch 64/93\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 623594.8750 - val_loss: 485394.9062\n",
      "Epoch 65/93\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 715880.0625 - val_loss: 461682.9375\n",
      "Epoch 66/93\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 339082.8750 - val_loss: 453025.2812\n",
      "Epoch 67/93\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 377271.5625 - val_loss: 460004.5312\n",
      "Epoch 68/93\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 428844.5938 - val_loss: 456543.6250\n",
      "Epoch 69/93\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 405038.0000 - val_loss: 455742.0938\n",
      "Epoch 70/93\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 412607.1875 - val_loss: 455600.7188\n",
      "Epoch 71/93\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 493343.1250 - val_loss: 452860.9688\n",
      "Epoch 72/93\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 384126.8750 - val_loss: 459275.3750\n",
      "Epoch 73/93\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 449572.9062 - val_loss: 458148.0938\n",
      "Epoch 74/93\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 482186.2500 - val_loss: 461810.9062\n",
      "Epoch 75/93\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 411093.9688 - val_loss: 456254.6250\n",
      "Epoch 76/93\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 511438.6875 - val_loss: 456817.0625\n",
      "Epoch 77/93\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 782902.2500 - val_loss: 451814.7812\n",
      "Epoch 78/93\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 359940.0000 - val_loss: 467026.3438\n",
      "Epoch 79/93\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 690461.6250 - val_loss: 457857.9062\n",
      "Epoch 80/93\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 1884414.1250 - val_loss: 456889.5625\n",
      "Epoch 81/93\n",
      "200/200 [==============================] - 18s 90ms/step - loss: 1739666.7500 - val_loss: 453769.0625\n",
      "Epoch 82/93\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 507695.4062 - val_loss: 454158.7812\n",
      "Epoch 83/93\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 1325510.7500 - val_loss: 452497.7812\n",
      "Epoch 84/93\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 613483.6250 - val_loss: 468161.9062\n",
      "Epoch 85/93\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 740612.3750 - val_loss: 471890.1562\n",
      "Epoch 86/93\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 813553.6875 - val_loss: 478666.3125\n",
      "Epoch 87/93\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 787526.5625 - val_loss: 484263.3125\n",
      "Epoch 88/93\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 694142.3125 - val_loss: 473723.0625\n",
      "Epoch 89/93\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 1089202.5000 - val_loss: 461530.0000\n",
      "Epoch 90/93\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 704895.5000 - val_loss: 493817.0312\n",
      "Epoch 91/93\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 473318.5625 - val_loss: 454531.3125\n",
      "Epoch 92/93\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 818792.8125 - val_loss: 453007.6875\n",
      "Epoch 93/93\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 597116.6250 - val_loss: 445166.7188\n",
      "[ 16 114]\n",
      "Epoch 1/114\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 427761.8438 - val_loss: 810944.6875\n",
      "Epoch 2/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 515567.3750 - val_loss: 810965.1250\n",
      "Epoch 3/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 586519.9375 - val_loss: 811837.3750\n",
      "Epoch 4/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 451884.9688 - val_loss: 816671.0000\n",
      "Epoch 5/114\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 358391.9688 - val_loss: 820052.8125\n",
      "Epoch 6/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 527581.9375 - val_loss: 845751.0000\n",
      "Epoch 7/114\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 484330.9062 - val_loss: 850464.9375\n",
      "Epoch 8/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 626801.1875 - val_loss: 885011.7500\n",
      "Epoch 9/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 845208.3750 - val_loss: 871097.3750\n",
      "Epoch 10/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 745857.3750 - val_loss: 898655.3750\n",
      "Epoch 11/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 539317.6875 - val_loss: 864546.0625\n",
      "Epoch 12/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 635560.7500 - val_loss: 906690.3125\n",
      "Epoch 13/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 529966.1250 - val_loss: 908055.5000\n",
      "Epoch 14/114\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 761257.0625 - val_loss: 846593.1250\n",
      "Epoch 15/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 452781.9062 - val_loss: 996322.5000\n",
      "Epoch 16/114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 17s 83ms/step - loss: 401918.5938 - val_loss: 880560.6875\n",
      "Epoch 17/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 462593.2812 - val_loss: 926629.8125\n",
      "Epoch 18/114\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 421295.1562 - val_loss: 955820.0625\n",
      "Epoch 19/114\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 436901.2812 - val_loss: 990872.4375\n",
      "Epoch 20/114\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 490977.0938 - val_loss: 1054799.6250\n",
      "Epoch 21/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 407488.3750 - val_loss: 935031.6250\n",
      "Epoch 22/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 457893.8125 - val_loss: 913390.2500\n",
      "Epoch 23/114\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 490595.8750 - val_loss: 869018.2500\n",
      "Epoch 24/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 428983.5312 - val_loss: 829568.7500\n",
      "Epoch 25/114\n",
      "200/200 [==============================] - 18s 90ms/step - loss: 531442.8125 - val_loss: 897231.8125\n",
      "Epoch 26/114\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 798332.6250 - val_loss: 1123263.7500\n",
      "Epoch 27/114\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 376513.5625 - val_loss: 968244.0000\n",
      "Epoch 28/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 683876.8125 - val_loss: 873397.8125\n",
      "Epoch 29/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 437810.7188 - val_loss: 867494.8750\n",
      "Epoch 30/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 3225880.0000 - val_loss: 841095.5625\n",
      "Epoch 31/114\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 517101.8750 - val_loss: 830312.2500\n",
      "Epoch 32/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 1300513.1250 - val_loss: 826277.8750\n",
      "Epoch 33/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 649203.1875 - val_loss: 834516.6875\n",
      "Epoch 34/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 742318.0625 - val_loss: 828798.0000\n",
      "Epoch 35/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 795323.6875 - val_loss: 839827.3125\n",
      "Epoch 36/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 765620.7500 - val_loss: 823416.1250\n",
      "Epoch 37/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 756481.6875 - val_loss: 818278.9375\n",
      "Epoch 38/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 1103630.3750 - val_loss: 891497.7500\n",
      "Epoch 39/114\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 684838.8750 - val_loss: 820591.6250\n",
      "Epoch 40/114\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 512941.2812 - val_loss: 818128.1250\n",
      "Epoch 41/114\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 822054.5000 - val_loss: 809166.0000\n",
      "Epoch 42/114\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 604938.2500 - val_loss: 826934.8125\n",
      "Epoch 43/114\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 1219901.3750 - val_loss: 858711.2500\n",
      "Epoch 44/114\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 3671375.0000 - val_loss: 862228.6875\n",
      "Epoch 45/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 43491764.0000 - val_loss: 838979.4375\n",
      "Epoch 46/114\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 3442335.7500 - val_loss: 815521.9375\n",
      "Epoch 47/114\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 1043852.8750 - val_loss: 815332.7500\n",
      "Epoch 48/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 698225.5000 - val_loss: 817468.2500\n",
      "Epoch 49/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 613506.3750 - val_loss: 802987.0625\n",
      "Epoch 50/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 607632.8125 - val_loss: 809687.1250\n",
      "Epoch 51/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 477861.3438 - val_loss: 827778.5625\n",
      "Epoch 52/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 415831.0000 - val_loss: 816548.3125\n",
      "Epoch 53/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 485962.1250 - val_loss: 826834.5000\n",
      "Epoch 54/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 552074.5000 - val_loss: 816951.8750\n",
      "Epoch 55/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 436041.4375 - val_loss: 808392.6250\n",
      "Epoch 56/114\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 337669.1250 - val_loss: 803351.1250\n",
      "Epoch 57/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 483402.6875 - val_loss: 814506.0625\n",
      "Epoch 58/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 469419.2812 - val_loss: 817206.8125\n",
      "Epoch 59/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 594627.9375 - val_loss: 806554.5000\n",
      "Epoch 60/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 813035.2500 - val_loss: 807818.5625\n",
      "Epoch 61/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 721705.4375 - val_loss: 814436.1250\n",
      "Epoch 62/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 668785.6875 - val_loss: 802272.1875\n",
      "Epoch 63/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 482279.9062 - val_loss: 814566.0625\n",
      "Epoch 64/114\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 612669.6875 - val_loss: 806162.6875\n",
      "Epoch 65/114\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 725308.3125 - val_loss: 802949.5000\n",
      "Epoch 66/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 339951.4062 - val_loss: 818649.3125\n",
      "Epoch 67/114\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 378831.8438 - val_loss: 821996.2500\n",
      "Epoch 68/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 428724.0312 - val_loss: 806325.0000\n",
      "Epoch 69/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 406980.2812 - val_loss: 816607.1250\n",
      "Epoch 70/114\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 414459.3750 - val_loss: 810012.2500\n",
      "Epoch 71/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 493068.8125 - val_loss: 797751.5625\n",
      "Epoch 72/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 385467.8125 - val_loss: 824333.6250\n",
      "Epoch 73/114\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 450320.0312 - val_loss: 812295.5625\n",
      "Epoch 74/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 484762.4375 - val_loss: 818815.5625\n",
      "Epoch 75/114\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 411499.3750 - val_loss: 826162.0625\n",
      "Epoch 76/114\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 514995.0938 - val_loss: 819541.0000\n",
      "Epoch 77/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 796204.1875 - val_loss: 823921.3750\n",
      "Epoch 78/114\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 345663.1250 - val_loss: 812642.1250\n",
      "Epoch 79/114\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 692634.3125 - val_loss: 810945.0000\n",
      "Epoch 80/114\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 1884440.3750 - val_loss: 806105.6250\n",
      "Epoch 81/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 1741967.2500 - val_loss: 806804.6250\n",
      "Epoch 82/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 502242.7500 - val_loss: 811987.8125\n",
      "Epoch 83/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 1325608.2500 - val_loss: 808548.8125\n",
      "Epoch 84/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 614049.2500 - val_loss: 810789.8750\n",
      "Epoch 85/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 741521.5000 - val_loss: 818396.0000\n",
      "Epoch 86/114\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 812729.4375 - val_loss: 814475.1250\n",
      "Epoch 87/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 779470.2500 - val_loss: 805418.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 695936.2500 - val_loss: 814042.7500\n",
      "Epoch 89/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 1092974.0000 - val_loss: 808349.4375\n",
      "Epoch 90/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 721302.3125 - val_loss: 858562.9375\n",
      "Epoch 91/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 461066.1562 - val_loss: 807106.3125\n",
      "Epoch 92/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 823848.7500 - val_loss: 812486.3750\n",
      "Epoch 93/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 596153.6250 - val_loss: 815472.8125\n",
      "Epoch 94/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 1219054.0000 - val_loss: 817130.6250\n",
      "Epoch 95/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 3666562.2500 - val_loss: 806435.0000\n",
      "Epoch 96/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 43463364.0000 - val_loss: 810582.0000\n",
      "Epoch 97/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 3487511.0000 - val_loss: 808184.3125\n",
      "Epoch 98/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 1006020.6250 - val_loss: 814724.1250\n",
      "Epoch 99/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 681249.9375 - val_loss: 808303.9375\n",
      "Epoch 100/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 604318.6875 - val_loss: 791289.3750\n",
      "Epoch 101/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 624411.7500 - val_loss: 793918.1250\n",
      "Epoch 102/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 467216.2812 - val_loss: 809863.0000\n",
      "Epoch 103/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 411431.1875 - val_loss: 811213.5000\n",
      "Epoch 104/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 485048.7188 - val_loss: 810233.1250\n",
      "Epoch 105/114\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 546185.5000 - val_loss: 813451.1875\n",
      "Epoch 106/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 431851.8750 - val_loss: 809058.1250\n",
      "Epoch 107/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 358520.2500 - val_loss: 814800.0625\n",
      "Epoch 108/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 444807.3125 - val_loss: 815103.0000\n",
      "Epoch 109/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 459228.1250 - val_loss: 807540.1875\n",
      "Epoch 110/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 603275.7500 - val_loss: 813078.6875\n",
      "Epoch 111/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 791092.9375 - val_loss: 809344.2500\n",
      "Epoch 112/114\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 729642.0625 - val_loss: 799280.0625\n",
      "Epoch 113/114\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 652309.8125 - val_loss: 801907.8750\n",
      "Epoch 114/114\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 486843.3750 - val_loss: 804265.6875\n",
      "[98 30]\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 30s 149ms/step - loss: 433423.0312 - val_loss: 537677.1875\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 508934.3750 - val_loss: 538786.0625\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 585707.3750 - val_loss: 534806.6875\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 18s 90ms/step - loss: 453844.7500 - val_loss: 534387.3125\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 361169.4688 - val_loss: 536799.7500\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 18s 90ms/step - loss: 530556.9375 - val_loss: 547546.0625\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 18s 90ms/step - loss: 484880.1250 - val_loss: 543085.0625\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 628903.1875 - val_loss: 547056.3750\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 841826.0625 - val_loss: 553135.6875\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 748836.0000 - val_loss: 589832.5000\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 538969.8125 - val_loss: 583956.8750\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 631919.0000 - val_loss: 562027.1875\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 531413.3750 - val_loss: 586669.6250\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 759036.6250 - val_loss: 543169.1875\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 450329.4375 - val_loss: 597153.5625\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 404036.0000 - val_loss: 608554.3125\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 465590.9062 - val_loss: 718973.6875\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 420759.9062 - val_loss: 652684.5000\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 435214.4062 - val_loss: 748342.9375\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 495267.2812 - val_loss: 758722.3125\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 408475.8438 - val_loss: 614921.7500\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 457843.1562 - val_loss: 632342.2500\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 492094.2812 - val_loss: 536356.7500\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 427893.8438 - val_loss: 536710.8750\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 530933.7500 - val_loss: 712329.3750\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 802331.7500 - val_loss: 705894.7500\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 372509.4688 - val_loss: 707991.6875\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 704355.1875 - val_loss: 647937.9375\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 410027.2500 - val_loss: 668381.6875\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 18s 90ms/step - loss: 3220007.0000 - val_loss: 625935.6250\n",
      "[116 144]\n",
      "Epoch 1/144\n",
      "200/200 [==============================] - 21s 107ms/step - loss: 434315.8750 - val_loss: 577390.8125\n",
      "Epoch 2/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 507925.3125 - val_loss: 578788.5625\n",
      "Epoch 3/144\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 585452.5625 - val_loss: 576518.1875\n",
      "Epoch 4/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 454301.1562 - val_loss: 576090.1250\n",
      "Epoch 5/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 359015.1562 - val_loss: 582813.5000\n",
      "Epoch 6/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 528182.5000 - val_loss: 594003.0625\n",
      "Epoch 7/144\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 484337.5312 - val_loss: 596998.5000\n",
      "Epoch 8/144\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 631218.4375 - val_loss: 612488.5000\n",
      "Epoch 9/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 839842.9375 - val_loss: 677735.7500\n",
      "Epoch 10/144\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 747608.0000 - val_loss: 615487.8750\n",
      "Epoch 11/144\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 543520.3750 - val_loss: 596689.0625\n",
      "Epoch 12/144\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 635787.6875 - val_loss: 585205.5000\n",
      "Epoch 13/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 530364.1250 - val_loss: 590934.3750\n",
      "Epoch 14/144\n",
      "200/200 [==============================] - 21s 107ms/step - loss: 757833.1875 - val_loss: 580986.8125\n",
      "Epoch 15/144\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 452921.1250 - val_loss: 594855.5625\n",
      "Epoch 16/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 400382.4688 - val_loss: 606445.3750\n",
      "Epoch 17/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 459362.1562 - val_loss: 683897.8750\n",
      "Epoch 18/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 422838.5625 - val_loss: 628918.7500\n",
      "Epoch 19/144\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 433654.1562 - val_loss: 752233.3750\n",
      "Epoch 20/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 492531.4688 - val_loss: 805378.0000\n",
      "Epoch 21/144\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 407838.9688 - val_loss: 774824.4375\n",
      "Epoch 22/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 456597.5625 - val_loss: 654159.3750\n",
      "Epoch 23/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 490331.6250 - val_loss: 578343.1250\n",
      "Epoch 24/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 428860.7188 - val_loss: 575381.4375\n",
      "Epoch 25/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 529407.1875 - val_loss: 591054.1875\n",
      "Epoch 26/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 793819.2500 - val_loss: 609883.5000\n",
      "Epoch 27/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 373969.6875 - val_loss: 685181.9375\n",
      "Epoch 28/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 707779.0625 - val_loss: 710986.5625\n",
      "Epoch 29/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 407153.0000 - val_loss: 698047.5625\n",
      "Epoch 30/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 3227842.5000 - val_loss: 644608.2500\n",
      "Epoch 31/144\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 513928.8750 - val_loss: 624741.0625\n",
      "Epoch 32/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 1336531.2500 - val_loss: 593840.5625\n",
      "Epoch 33/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 611176.6250 - val_loss: 609018.7500\n",
      "Epoch 34/144\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 743351.6875 - val_loss: 610335.8750\n",
      "Epoch 35/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 799623.7500 - val_loss: 593335.3750\n",
      "Epoch 36/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 765117.0625 - val_loss: 627186.0000\n",
      "Epoch 37/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 755390.3750 - val_loss: 615439.5000\n",
      "Epoch 38/144\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 1112861.3750 - val_loss: 580530.4375\n",
      "Epoch 39/144\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 679872.6250 - val_loss: 572989.6250\n",
      "Epoch 40/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 508518.5938 - val_loss: 578809.8750\n",
      "Epoch 41/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 828142.6250 - val_loss: 586298.5625\n",
      "Epoch 42/144\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 605058.6250 - val_loss: 571355.7500\n",
      "Epoch 43/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 1215993.7500 - val_loss: 640840.8125\n",
      "Epoch 44/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 3670951.2500 - val_loss: 581473.3750\n",
      "Epoch 45/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 43484164.0000 - val_loss: 588581.9375\n",
      "Epoch 46/144\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 3448018.0000 - val_loss: 581315.6250\n",
      "Epoch 47/144\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 1043273.2500 - val_loss: 586958.1875\n",
      "Epoch 48/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 698399.0625 - val_loss: 586693.9375\n",
      "Epoch 49/144\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 613312.5000 - val_loss: 555704.2500\n",
      "Epoch 50/144\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 607463.3750 - val_loss: 567947.2500\n",
      "Epoch 51/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 479076.1250 - val_loss: 581898.5000\n",
      "Epoch 52/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 417924.4062 - val_loss: 585704.4375\n",
      "Epoch 53/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 483706.9688 - val_loss: 598958.2500\n",
      "Epoch 54/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 556596.0625 - val_loss: 580261.3125\n",
      "Epoch 55/144\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 422151.3125 - val_loss: 580206.9375\n",
      "Epoch 56/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 346253.0938 - val_loss: 578683.4375\n",
      "Epoch 57/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 478375.1250 - val_loss: 597318.7500\n",
      "Epoch 58/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 475683.6875 - val_loss: 586200.8750\n",
      "Epoch 59/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 591119.7500 - val_loss: 580804.0625\n",
      "Epoch 60/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 813563.6875 - val_loss: 581634.1250\n",
      "Epoch 61/144\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 720359.3750 - val_loss: 578717.4375\n",
      "Epoch 62/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 667484.5625 - val_loss: 581933.7500\n",
      "Epoch 63/144\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 473216.1562 - val_loss: 580413.3125\n",
      "Epoch 64/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 617715.2500 - val_loss: 583451.6250\n",
      "Epoch 65/144\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 719628.7500 - val_loss: 574310.5625\n",
      "Epoch 66/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 339870.3750 - val_loss: 566784.8125\n",
      "Epoch 67/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 375311.0000 - val_loss: 578369.7500\n",
      "Epoch 68/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 427124.3750 - val_loss: 574590.1875\n",
      "Epoch 69/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 406715.1875 - val_loss: 566853.3750\n",
      "Epoch 70/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 411772.6250 - val_loss: 569253.4375\n",
      "Epoch 71/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 495376.7188 - val_loss: 578345.0000\n",
      "Epoch 72/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 383910.8438 - val_loss: 574659.1875\n",
      "Epoch 73/144\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 448401.7500 - val_loss: 581075.3750\n",
      "Epoch 74/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 484516.3125 - val_loss: 573694.9375\n",
      "Epoch 75/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 411988.9062 - val_loss: 575158.8750\n",
      "Epoch 76/144\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 509844.4688 - val_loss: 585208.1250\n",
      "Epoch 77/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 794010.6250 - val_loss: 584520.3750\n",
      "Epoch 78/144\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 347921.5625 - val_loss: 598347.1250\n",
      "Epoch 79/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 693293.2500 - val_loss: 606697.9375\n",
      "Epoch 80/144\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 1883256.0000 - val_loss: 593828.1875\n",
      "Epoch 81/144\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 1737336.1250 - val_loss: 576754.1875\n",
      "Epoch 82/144\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 501243.4062 - val_loss: 572294.1250\n",
      "Epoch 83/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 1323586.1250 - val_loss: 565840.8125\n",
      "Epoch 84/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 613969.2500 - val_loss: 574338.5000\n",
      "Epoch 85/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 736762.6250 - val_loss: 571256.2500\n",
      "Epoch 86/144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 20s 98ms/step - loss: 810553.5000 - val_loss: 570960.0625\n",
      "Epoch 87/144\n",
      "200/200 [==============================] - 22s 108ms/step - loss: 778788.5000 - val_loss: 573135.1250\n",
      "Epoch 88/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 691465.3750 - val_loss: 574825.3750\n",
      "Epoch 89/144\n",
      "200/200 [==============================] - 21s 106ms/step - loss: 1089674.7500 - val_loss: 573775.2500\n",
      "Epoch 90/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 716376.0000 - val_loss: 604609.8750\n",
      "Epoch 91/144\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 454502.2500 - val_loss: 564647.3125\n",
      "Epoch 92/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 817386.5625 - val_loss: 574712.5625\n",
      "Epoch 93/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 598483.1250 - val_loss: 568428.0000\n",
      "Epoch 94/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 1218817.0000 - val_loss: 568817.9375\n",
      "Epoch 95/144\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 3661756.7500 - val_loss: 566661.6250\n",
      "Epoch 96/144\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 43461840.0000 - val_loss: 569032.0625\n",
      "Epoch 97/144\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 3486615.2500 - val_loss: 572465.7500\n",
      "Epoch 98/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 1003190.5625 - val_loss: 582402.0625\n",
      "Epoch 99/144\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 682505.1875 - val_loss: 567139.0625\n",
      "Epoch 100/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 602470.7500 - val_loss: 548075.6875\n",
      "Epoch 101/144\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 622098.3750 - val_loss: 549859.4375\n",
      "Epoch 102/144\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 466782.7188 - val_loss: 585170.3750\n",
      "Epoch 103/144\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 410726.6250 - val_loss: 578366.8125\n",
      "Epoch 104/144\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 479127.8750 - val_loss: 575785.5625\n",
      "Epoch 105/144\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 541168.6250 - val_loss: 584230.6250\n",
      "Epoch 106/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 426544.3750 - val_loss: 579270.4375\n",
      "Epoch 107/144\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 358116.1250 - val_loss: 579755.1250\n",
      "Epoch 108/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 441051.8438 - val_loss: 583862.6250\n",
      "Epoch 109/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 458849.3125 - val_loss: 575475.1875\n",
      "Epoch 110/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 594571.7500 - val_loss: 575776.8750\n",
      "Epoch 111/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 792059.3750 - val_loss: 572919.9375\n",
      "Epoch 112/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 806198.6250 - val_loss: 565510.8125\n",
      "Epoch 113/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 570934.3125 - val_loss: 576766.8125\n",
      "Epoch 114/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 469734.5938 - val_loss: 580065.4375\n",
      "Epoch 115/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 608724.8750 - val_loss: 587010.4375\n",
      "Epoch 116/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 704686.8125 - val_loss: 577055.7500\n",
      "Epoch 117/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 325812.8750 - val_loss: 575312.2500\n",
      "Epoch 118/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 370884.8750 - val_loss: 583784.0625\n",
      "Epoch 119/144\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 420345.8438 - val_loss: 568898.8125\n",
      "Epoch 120/144\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 396179.9688 - val_loss: 565695.9375\n",
      "Epoch 121/144\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 400332.2812 - val_loss: 567876.5000\n",
      "Epoch 122/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 475204.8750 - val_loss: 573194.7500\n",
      "Epoch 123/144\n",
      "200/200 [==============================] - 18s 90ms/step - loss: 389091.8438 - val_loss: 564858.9375\n",
      "Epoch 124/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 432248.1250 - val_loss: 570626.5625\n",
      "Epoch 125/144\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 478645.5312 - val_loss: 569105.0000\n",
      "Epoch 126/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 412946.6875 - val_loss: 565733.3750\n",
      "Epoch 127/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 490888.1250 - val_loss: 591000.0625\n",
      "Epoch 128/144\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 785017.1250 - val_loss: 578556.4375\n",
      "Epoch 129/144\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 342205.7188 - val_loss: 577557.8750\n",
      "Epoch 130/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 682251.0625 - val_loss: 578163.3750\n",
      "Epoch 131/144\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 1869045.7500 - val_loss: 580130.0625\n",
      "Epoch 132/144\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 1735728.7500 - val_loss: 573164.8125\n",
      "Epoch 133/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 619665.0000 - val_loss: 562946.7500\n",
      "Epoch 134/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 1213485.8750 - val_loss: 563098.5625\n",
      "Epoch 135/144\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 590980.1875 - val_loss: 573711.9375\n",
      "Epoch 136/144\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 856256.3125 - val_loss: 575095.9375\n",
      "Epoch 137/144\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 707975.0625 - val_loss: 568324.5000\n",
      "Epoch 138/144\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 750562.3750 - val_loss: 562623.0000\n",
      "Epoch 139/144\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 739863.6250 - val_loss: 567022.1875\n",
      "Epoch 140/144\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 1027503.7500 - val_loss: 560151.1250\n",
      "Epoch 141/144\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 707959.9375 - val_loss: 558826.9375\n",
      "Epoch 142/144\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 444806.7500 - val_loss: 556821.6875\n",
      "Epoch 143/144\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 847589.2500 - val_loss: 580247.3750\n",
      "Epoch 144/144\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 553411.4375 - val_loss: 564815.9375\n",
      "[ 6 36]\n",
      "Epoch 1/36\n",
      "200/200 [==============================] - 15s 75ms/step - loss: 436016.0312 - val_loss: 277830.6562\n",
      "Epoch 2/36\n",
      "200/200 [==============================] - 14s 72ms/step - loss: 506966.1562 - val_loss: 276460.9062\n",
      "Epoch 3/36\n",
      "200/200 [==============================] - 14s 71ms/step - loss: 583745.6875 - val_loss: 273711.7188\n",
      "Epoch 4/36\n",
      "200/200 [==============================] - 14s 70ms/step - loss: 452640.9688 - val_loss: 269580.0938\n",
      "Epoch 5/36\n",
      "200/200 [==============================] - 14s 72ms/step - loss: 373931.4062 - val_loss: 275153.9062\n",
      "Epoch 6/36\n",
      "200/200 [==============================] - 14s 72ms/step - loss: 517159.7188 - val_loss: 306822.9062\n",
      "Epoch 7/36\n",
      "200/200 [==============================] - 14s 71ms/step - loss: 484006.1875 - val_loss: 302784.9062\n",
      "Epoch 8/36\n",
      "200/200 [==============================] - 14s 71ms/step - loss: 628378.9375 - val_loss: 332517.7500\n",
      "Epoch 9/36\n",
      "200/200 [==============================] - 14s 71ms/step - loss: 821786.3750 - val_loss: 326389.6875\n",
      "Epoch 10/36\n",
      "200/200 [==============================] - 15s 73ms/step - loss: 767818.7500 - val_loss: 288359.9688\n",
      "Epoch 11/36\n",
      "200/200 [==============================] - 15s 73ms/step - loss: 540499.6875 - val_loss: 288835.8750\n",
      "Epoch 12/36\n",
      "200/200 [==============================] - 15s 74ms/step - loss: 633020.2500 - val_loss: 289812.5312\n",
      "Epoch 13/36\n",
      "200/200 [==============================] - 15s 74ms/step - loss: 530912.6875 - val_loss: 300602.9062\n",
      "Epoch 14/36\n",
      "200/200 [==============================] - 15s 76ms/step - loss: 757708.5000 - val_loss: 292218.0938\n",
      "Epoch 15/36\n",
      "200/200 [==============================] - 15s 74ms/step - loss: 455339.8438 - val_loss: 330333.0938\n",
      "Epoch 16/36\n",
      "200/200 [==============================] - 15s 74ms/step - loss: 401848.9688 - val_loss: 306158.7812\n",
      "Epoch 17/36\n",
      "200/200 [==============================] - 15s 75ms/step - loss: 463830.9688 - val_loss: 343023.7188\n",
      "Epoch 18/36\n",
      "200/200 [==============================] - 15s 77ms/step - loss: 423919.4062 - val_loss: 315421.6562\n",
      "Epoch 19/36\n",
      "200/200 [==============================] - 15s 76ms/step - loss: 436747.6250 - val_loss: 304451.4375\n",
      "Epoch 20/36\n",
      "200/200 [==============================] - 15s 76ms/step - loss: 494110.7188 - val_loss: 308450.0625\n",
      "Epoch 21/36\n",
      "200/200 [==============================] - 15s 77ms/step - loss: 407358.8438 - val_loss: 319522.4688\n",
      "Epoch 22/36\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 455353.1875 - val_loss: 331472.7812\n",
      "Epoch 23/36\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 490297.5312 - val_loss: 284428.0938\n",
      "Epoch 24/36\n",
      "200/200 [==============================] - 15s 77ms/step - loss: 428688.9688 - val_loss: 292827.0938\n",
      "Epoch 25/36\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 530465.9375 - val_loss: 317614.0938\n",
      "Epoch 26/36\n",
      "200/200 [==============================] - 15s 77ms/step - loss: 794124.0625 - val_loss: 342472.3438\n",
      "Epoch 27/36\n",
      "200/200 [==============================] - 15s 77ms/step - loss: 373194.4062 - val_loss: 360543.0000\n",
      "Epoch 28/36\n",
      "200/200 [==============================] - 15s 75ms/step - loss: 710075.5000 - val_loss: 362224.1250\n",
      "Epoch 29/36\n",
      "200/200 [==============================] - 15s 76ms/step - loss: 408215.4688 - val_loss: 368827.0938\n",
      "Epoch 30/36\n",
      "200/200 [==============================] - 15s 73ms/step - loss: 3214963.5000 - val_loss: 307408.2500\n",
      "Epoch 31/36\n",
      "200/200 [==============================] - 15s 74ms/step - loss: 523128.1562 - val_loss: 296959.4062\n",
      "Epoch 32/36\n",
      "200/200 [==============================] - 15s 73ms/step - loss: 1331548.1250 - val_loss: 277907.3750\n",
      "Epoch 33/36\n",
      "200/200 [==============================] - 15s 73ms/step - loss: 613202.0625 - val_loss: 273684.3750\n",
      "Epoch 34/36\n",
      "200/200 [==============================] - 14s 71ms/step - loss: 743684.0000 - val_loss: 282409.0938\n",
      "Epoch 35/36\n",
      "200/200 [==============================] - 14s 72ms/step - loss: 794715.1875 - val_loss: 295191.7500\n",
      "Epoch 36/36\n",
      "200/200 [==============================] - 14s 72ms/step - loss: 771120.2500 - val_loss: 284182.9062\n"
     ]
    }
   ],
   "source": [
    "print(\"initialisation...............\")\n",
    "allLosses=np.array([])\n",
    "allLosses=FireflyIteration(fireflies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin de initialisation des paramère\n"
     ]
    }
   ],
   "source": [
    "print(\"fin de initialisation des paramère\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************  voila les paramètre (loss) des firefly a initialisé**********************\n",
      "[759.2343645719959, 750.620628101023, 751.869245692173, 795.8956073262817, 747.925084446167, 796.4490197497768]\n",
      "firefly [ epochs =  36 , filter =  86 ] ==> Loss (lumunisoté :) 759.2343645719959\n",
      "firefly [ epochs =  93 , filter =  65 ] ==> Loss (lumunisoté :) 750.620628101023\n",
      "firefly [ epochs =  114 , filter =  16 ] ==> Loss (lumunisoté :) 751.869245692173\n",
      "firefly [ epochs =  30 , filter =  98 ] ==> Loss (lumunisoté :) 795.8956073262817\n",
      "firefly [ epochs =  144 , filter =  116 ] ==> Loss (lumunisoté :) 747.925084446167\n",
      "firefly [ epochs =  36 , filter =  6 ] ==> Loss (lumunisoté :) 796.4490197497768\n"
     ]
    }
   ],
   "source": [
    "print(\"************************************  voila les paramètre (loss) des firefly a initialisé**********************\")\n",
    "print(allLosses)\n",
    "for i in range(numberFirefly):\n",
    "    print(\"firefly [ epochs = \",fireflies[i][1],\", filter = \" ,fireflies[i][0], \"] ==> Loss (lumunisoté :)\",allLosses[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>le lancement de l'optimisation A l'aide de l'algorithme Firefly</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import Image\n",
    "Image(\"DPL3.png\",width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* class Firefly ********\n"
     ]
    }
   ],
   "source": [
    "print(\"********* class Firefly ********\")\n",
    "\n",
    "class Firefly:\n",
    "\n",
    "    def __init__(self, position,brightness,function):\n",
    "        self.func = function                         # Choose the benchmark algorithm to run\n",
    "        self.position = position\n",
    "        self.brightness = brightness\n",
    "        #self.update_brightness()\n",
    "\n",
    "    # the best fit is 0\n",
    "    def update_brightness(self):\n",
    "        self.brightness = -self.func(self.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireflyOptimizer:\n",
    "\n",
    "    def __init__(self, obj, **kwargs):\n",
    "        self.population_size = int(kwargs.get('population_size', 4))\n",
    "        self.problem_dim = kwargs.get('problem_dim', 2)\n",
    "        self.min_bound = kwargs.get('min_bound', 1)\n",
    "        self.max_bound = kwargs.get('max_bound', 30)\n",
    "        self.generations = kwargs.get('generations', 2)\n",
    "        self.population = self._population(self.population_size, self.problem_dim, self.min_bound, self.max_bound)\n",
    "        self.gamma = kwargs.get('gamma', 0.95)  # absorption coefficient\n",
    "        self.alpha = kwargs.get('alpha', 0.25)  # randomness [0,1]\n",
    "        self.beta_init = kwargs.get('beta_init', 1)\n",
    "        self.beta_min = kwargs.get('beta_min', 0.2)\n",
    "        self.optimization_benchmark = obj\n",
    "        self.dtime = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _population(population_size, problem_dim, min_bound, max_bound):\n",
    "        population = []\n",
    "        for i in range(population_size):\n",
    "            population.append(Firefly(problem_dim, min_bound, max_bound))\n",
    "        return population\n",
    "\n",
    "    def step(self):\n",
    "        self.population.sort(key=operator.attrgetter('brightness'), reverse=True)\n",
    "        self._modify_alpha()\n",
    "        tmp_population = self.population\n",
    "        for i in range(self.population_size):\n",
    "            for j in range(self.population_size):\n",
    "                if self.population[i].brightness > tmp_population[j].brightness:\n",
    "                    r = math.sqrt(np.sum((self.population[i].position - tmp_population[j].position) ** 2))\n",
    "                    beta = (self.beta_init - self.beta_min) * math.exp(-self.gamma * r ** 2) + self.beta_min\n",
    "                    tmp = self.alpha * (np.random.random_sample((1, self.problem_dim))[0] - 0.5) * (\n",
    "                            self.max_bound - self.min_bound)\n",
    "                    self.population[j].position = self.check_position(\n",
    "                        self.population[i].position * (1 - beta) + tmp_population[\n",
    "                            j].position * beta + tmp)\n",
    "                    self.population[j].update_brightness()\n",
    "\n",
    "        # Changing the position of Brightness so the population will shift to other locations\n",
    "\n",
    "        # self.population[0].position = generate_population(1, self.problem_dim, self.min_bound, self.max_bound)[0]\n",
    "        # self.population[0].update_brightness()\n",
    "\n",
    "    # Best placement for each Generation\n",
    "    def run_firefly(self, population):\n",
    "        self.population = population\n",
    "        start = time.time()\n",
    "\n",
    "        self.res = {}\n",
    "        for t in range(self.generations):\n",
    "            self.res[t] = ('Generation %s, best fitness %s' % (t, self.population[0].brightness))\n",
    "            self.step()\n",
    "        end = time.time()\n",
    "        self.dtime = end - start\n",
    "        self.dtime = float(str(end - start)[:4])\n",
    "        # \"\"\" psg.popup_scrolled('FA Results\\n',\n",
    "        #                  self.res,\n",
    "        #                 '\\nAlgorithm run time: ', self.dtime,\n",
    "        #                '\\nAverage CPU usage: ', psutil.cpu_percent(), size=[50, 30])\"\"\"\n",
    "\n",
    "        self.population.sort(key=operator.attrgetter('brightness'), reverse=True)\n",
    "        return self.population[0].brightness, self.population[0].position\n",
    "    def check_position(self, position):\n",
    "        position[position > self.max_bound] = self.max_bound\n",
    "        position[position < self.min_bound] = self.min_bound\n",
    "        return position\n",
    "\n",
    "    def _modify_alpha(self):\n",
    "        delta = 1 - (10 ** (-4) / 0.9) ** (1 / self.generations)\n",
    "        self.alpha = (1 - delta) * self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lstm_firefly(firefly):\n",
    "    print(\"la fonction objective de l'algorithme de firefly va avoir une luciole en paramère et\"\n",
    "          \"retourne le taux de perte de modèle basé sur les paramètre de cette luciole\")\n",
    "    epochs=int(firefly[1])\n",
    "    filters=int(firefly[0])\n",
    "    #x_test,y_test=generate_x_test()\n",
    "    dropout = 0.0\n",
    "    simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(\n",
    "        128, input_shape=X_train_w.shape[-2:], dropout=dropout),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    simple_lstm_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "\n",
    "\n",
    "    EVALUATION_INTERVAL = 200\n",
    "    EPOCHS = epochs\n",
    "\n",
    "    model_history = simple_lstm_model.fit(train_data, epochs=EPOCHS,\n",
    "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                          validation_data=val_data, validation_steps=filters)  # ,callbacks=[tensorboard_callback]) #Uncomment this line for tensorboard support\n",
    "\n",
    "\n",
    "    yhat = simple_lstm_model.predict(X_test_w).reshape(1, -1)[0]\n",
    "    rmse = sqrt(mean_squared_error(y_test, yhat))\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation des firefly....;\n"
     ]
    }
   ],
   "source": [
    "print(\"creation des firefly....;\")\n",
    "population = [Firefly(fireflies[i],allLosses[i],Lstm_firefly)for i in range(len(fireflies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lancement algorithme de firefly..........\n",
      "l'algorithme peut prendre un peu de temps merci de patienter\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 433853.1250 - val_loss: 591027.4375\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 509254.5938 - val_loss: 591018.8750\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 587535.4375 - val_loss: 597184.8750\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 450124.6250 - val_loss: 593759.8125\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 358455.3750 - val_loss: 601606.8125\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 42s 212ms/step - loss: 528525.6250 - val_loss: 620909.2500\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 481944.3750 - val_loss: 601882.8750\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 632647.0625 - val_loss: 651553.8125\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 816392.7500 - val_loss: 614520.4375\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 764048.3750 - val_loss: 616185.1875\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 542122.9375 - val_loss: 595193.9375\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 630544.0625 - val_loss: 617108.8750\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 529160.1875 - val_loss: 604618.8125\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 756119.1250 - val_loss: 588915.5625\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 450965.5312 - val_loss: 614460.1875\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 400228.4062 - val_loss: 622949.2500\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 455256.8750 - val_loss: 623458.0625\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 419003.4375 - val_loss: 627170.5000\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 428891.2812 - val_loss: 640536.0625\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 491796.7188 - val_loss: 631205.8750\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 406630.5938 - val_loss: 607196.6875\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 34s 172ms/step - loss: 454952.5938 - val_loss: 714411.9375\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 42s 209ms/step - loss: 488870.6250 - val_loss: 600817.4375\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 426541.4375 - val_loss: 608525.6250\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 525670.0625 - val_loss: 614443.5625\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 791358.8125 - val_loss: 617549.2500\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 373524.4062 - val_loss: 631812.1875\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 33s 166ms/step - loss: 704373.3750 - val_loss: 690480.4375\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 406528.1562 - val_loss: 708221.7500\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 31s 155ms/step - loss: 3229036.5000 - val_loss: 609279.2500\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 35s 175ms/step - loss: 434363.0312 - val_loss: 633465.2500\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 507899.1562 - val_loss: 635728.8750\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 585438.3125 - val_loss: 633141.3125\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 456007.0000 - val_loss: 636831.4375\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 357762.5312 - val_loss: 636431.1875\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 530137.6250 - val_loss: 667185.8750\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 483919.8125 - val_loss: 656132.6250\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 35s 174ms/step - loss: 631356.2500 - val_loss: 657649.0625\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 838653.2500 - val_loss: 692145.0625\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 746317.3750 - val_loss: 675782.1875\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 543295.3750 - val_loss: 652644.2500\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 633966.3750 - val_loss: 665905.4375\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 529688.9375 - val_loss: 660125.5625\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 37s 183ms/step - loss: 760711.9375 - val_loss: 644798.9375\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 448983.4688 - val_loss: 781613.0625\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 404902.8438 - val_loss: 694768.4375\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 469586.5938 - val_loss: 713047.0625\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 423087.7188 - val_loss: 698005.1250\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 438691.7188 - val_loss: 745810.3750\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 494344.5312 - val_loss: 689317.1250\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 408984.8438 - val_loss: 704184.7500\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 455762.1250 - val_loss: 715538.6250\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 494200.4688 - val_loss: 690810.5625\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 429252.8438 - val_loss: 691698.1250\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 531320.3750 - val_loss: 717473.8750\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 28s 138ms/step - loss: 798130.8125 - val_loss: 724119.1875\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 372220.2812 - val_loss: 703760.1875\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 34s 172ms/step - loss: 710701.1875 - val_loss: 718194.2500\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 410898.3750 - val_loss: 731817.4375\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 42s 211ms/step - loss: 3229077.0000 - val_loss: 719719.8125\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 433412.4062 - val_loss: 319753.0938\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 508828.7500 - val_loss: 320529.5000\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 587864.0625 - val_loss: 330242.5000\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 450001.9062 - val_loss: 313010.0000\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 358915.9688 - val_loss: 315784.7812\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 526857.9375 - val_loss: 353012.5000\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 483320.9688 - val_loss: 336065.2500\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 626371.8125 - val_loss: 361993.6562\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 817757.1875 - val_loss: 342504.1562\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 764934.5000 - val_loss: 339786.0000\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 542872.5625 - val_loss: 335261.7812\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 629839.6250 - val_loss: 420807.4375\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 528039.8750 - val_loss: 371169.7188\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 759459.7500 - val_loss: 350047.5312\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 446064.3750 - val_loss: 397490.6875\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 400388.3125 - val_loss: 337396.6562\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 453614.2500 - val_loss: 391853.0625\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 422389.5625 - val_loss: 396949.1875\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 432678.1250 - val_loss: 362996.5625\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 494572.3750 - val_loss: 325250.3125\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 404797.1562 - val_loss: 366759.7188\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 42s 211ms/step - loss: 456592.4062 - val_loss: 382792.0312\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 486372.0312 - val_loss: 335395.2188\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 34s 172ms/step - loss: 425606.9688 - val_loss: 344128.1562\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 33s 167ms/step - loss: 528146.7500 - val_loss: 347688.5000\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 793588.9375 - val_loss: 339887.0625\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 374061.9062 - val_loss: 361020.0625\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 707535.2500 - val_loss: 422757.2188\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 405260.4062 - val_loss: 377494.8438\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 3220953.2500 - val_loss: 326767.4062\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/29\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 428894.4375 - val_loss: 607514.0625\n",
      "Epoch 2/29\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 513473.8750 - val_loss: 608964.1875\n",
      "Epoch 3/29\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 585840.6250 - val_loss: 608777.0000\n",
      "Epoch 4/29\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 453208.7188 - val_loss: 608827.3125\n",
      "Epoch 5/29\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 359162.1875 - val_loss: 615967.9375\n",
      "Epoch 6/29\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 528787.6875 - val_loss: 643677.0625\n",
      "Epoch 7/29\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 484725.5312 - val_loss: 623445.3750\n",
      "Epoch 8/29\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 632866.0000 - val_loss: 658189.3750\n",
      "Epoch 9/29\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 837960.3125 - val_loss: 700735.1250\n",
      "Epoch 10/29\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 746042.2500 - val_loss: 653373.1875\n",
      "Epoch 11/29\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 539426.1875 - val_loss: 666812.9375\n",
      "Epoch 12/29\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 634228.1250 - val_loss: 684442.2500\n",
      "Epoch 13/29\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 528801.5000 - val_loss: 699432.5625\n",
      "Epoch 14/29\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 759501.9375 - val_loss: 696284.6875\n",
      "Epoch 15/29\n",
      "200/200 [==============================] - 47s 237ms/step - loss: 447540.9062 - val_loss: 757719.7500\n",
      "Epoch 16/29\n",
      "200/200 [==============================] - 47s 236ms/step - loss: 404258.5625 - val_loss: 702899.5625\n",
      "Epoch 17/29\n",
      "200/200 [==============================] - 47s 237ms/step - loss: 467739.3125 - val_loss: 690077.7500\n",
      "Epoch 18/29\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 425014.5312 - val_loss: 693809.8125\n",
      "Epoch 19/29\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 436445.6875 - val_loss: 743117.3750\n",
      "Epoch 20/29\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 495530.4375 - val_loss: 714590.9375\n",
      "Epoch 21/29\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 405101.6875 - val_loss: 682923.2500\n",
      "Epoch 22/29\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 458814.3125 - val_loss: 689196.2500\n",
      "Epoch 23/29\n",
      "200/200 [==============================] - 42s 211ms/step - loss: 489806.0312 - val_loss: 639568.8125\n",
      "Epoch 24/29\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 428454.2500 - val_loss: 643399.1250\n",
      "Epoch 25/29\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 529371.5000 - val_loss: 680525.6250\n",
      "Epoch 26/29\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 798538.1875 - val_loss: 673519.4375\n",
      "Epoch 27/29\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 372680.6875 - val_loss: 679128.9375\n",
      "Epoch 28/29\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 713735.5000 - val_loss: 690099.0000\n",
      "Epoch 29/29\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 409831.8750 - val_loss: 740939.8125\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/29\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 434991.1875 - val_loss: 645639.3125\n",
      "Epoch 2/29\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 505872.6875 - val_loss: 646461.7500\n",
      "Epoch 3/29\n",
      "200/200 [==============================] - 38s 190ms/step - loss: 586980.8125 - val_loss: 648128.3125\n",
      "Epoch 4/29\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 453169.8438 - val_loss: 651010.7500\n",
      "Epoch 5/29\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 358343.5312 - val_loss: 656743.6250\n",
      "Epoch 6/29\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 525207.7500 - val_loss: 689051.6250\n",
      "Epoch 7/29\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 483773.5625 - val_loss: 686762.5625\n",
      "Epoch 8/29\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 630746.6250 - val_loss: 706890.2500\n",
      "Epoch 9/29\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 815296.6250 - val_loss: 701602.5625\n",
      "Epoch 10/29\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 763224.5000 - val_loss: 695250.0625\n",
      "Epoch 11/29\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 543560.2500 - val_loss: 684817.6250\n",
      "Epoch 12/29\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 628834.3750 - val_loss: 708104.8750\n",
      "Epoch 13/29\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 530933.9375 - val_loss: 722266.3125\n",
      "Epoch 14/29\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 761541.6875 - val_loss: 662836.0625\n",
      "Epoch 15/29\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 449214.1875 - val_loss: 708521.8750\n",
      "Epoch 16/29\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 401624.2812 - val_loss: 737332.1250\n",
      "Epoch 17/29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 41s 206ms/step - loss: 460163.2500 - val_loss: 748950.1875\n",
      "Epoch 18/29\n",
      "200/200 [==============================] - 47s 234ms/step - loss: 422771.8438 - val_loss: 705787.0625\n",
      "Epoch 19/29\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 433250.0000 - val_loss: 754760.2500\n",
      "Epoch 20/29\n",
      "200/200 [==============================] - 38s 191ms/step - loss: 495411.2812 - val_loss: 806803.6250\n",
      "Epoch 21/29\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 408570.3750 - val_loss: 735472.6875\n",
      "Epoch 22/29\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 457054.1562 - val_loss: 730697.0625\n",
      "Epoch 23/29\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 491164.2812 - val_loss: 656473.7500\n",
      "Epoch 24/29\n",
      "200/200 [==============================] - 38s 191ms/step - loss: 427784.4688 - val_loss: 677751.8750\n",
      "Epoch 25/29\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 529863.7500 - val_loss: 659480.6875\n",
      "Epoch 26/29\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 794560.7500 - val_loss: 696129.8750\n",
      "Epoch 27/29\n",
      "200/200 [==============================] - 31s 153ms/step - loss: 374482.1562 - val_loss: 701990.2500\n",
      "Epoch 28/29\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 708332.2500 - val_loss: 834810.7500\n",
      "Epoch 29/29\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 408196.8438 - val_loss: 844390.7500\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/29\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 434157.4375 - val_loss: 606950.8750\n",
      "Epoch 2/29\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 507168.3750 - val_loss: 608053.0625\n",
      "Epoch 3/29\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 584923.9375 - val_loss: 609332.5625\n",
      "Epoch 4/29\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 453687.2500 - val_loss: 612524.6875\n",
      "Epoch 5/29\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 370425.1875 - val_loss: 637752.7500\n",
      "Epoch 6/29\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 514159.0000 - val_loss: 663253.5000\n",
      "Epoch 7/29\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 484848.7188 - val_loss: 624915.3125\n",
      "Epoch 8/29\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 632503.5000 - val_loss: 649530.9375\n",
      "Epoch 9/29\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 838066.5000 - val_loss: 626606.5625\n",
      "Epoch 10/29\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 746837.3750 - val_loss: 645325.8750\n",
      "Epoch 11/29\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 539977.9375 - val_loss: 621462.9375\n",
      "Epoch 12/29\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 635561.5625 - val_loss: 632349.5625\n",
      "Epoch 13/29\n",
      "200/200 [==============================] - 38s 191ms/step - loss: 529624.5625 - val_loss: 625008.1250\n",
      "Epoch 14/29\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 761214.1875 - val_loss: 610542.1875\n",
      "Epoch 15/29\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 447284.4688 - val_loss: 638948.4375\n",
      "Epoch 16/29\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 402417.1875 - val_loss: 622150.1250\n",
      "Epoch 17/29\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 460449.4688 - val_loss: 648285.5000\n",
      "Epoch 18/29\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 423646.6250 - val_loss: 700383.6250\n",
      "Epoch 19/29\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 434135.0000 - val_loss: 800353.3750\n",
      "Epoch 20/29\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 492250.1562 - val_loss: 813629.1250\n",
      "Epoch 21/29\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 406775.5938 - val_loss: 715640.6875\n",
      "Epoch 22/29\n",
      "200/200 [==============================] - 38s 192ms/step - loss: 457076.0000 - val_loss: 745882.0000\n",
      "Epoch 23/29\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 490514.4062 - val_loss: 622063.3750\n",
      "Epoch 24/29\n",
      "200/200 [==============================] - 38s 190ms/step - loss: 428482.2500 - val_loss: 622843.3750\n",
      "Epoch 25/29\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 526283.7500 - val_loss: 634468.9375\n",
      "Epoch 26/29\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 793509.6250 - val_loss: 644893.9375\n",
      "Epoch 27/29\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 374342.8438 - val_loss: 635931.6250\n",
      "Epoch 28/29\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 711314.5000 - val_loss: 684356.5000\n",
      "Epoch 29/29\n",
      "200/200 [==============================] - 38s 191ms/step - loss: 407782.0938 - val_loss: 781720.0625\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/29\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 435674.4688 - val_loss: 606832.5000\n",
      "Epoch 2/29\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 507475.9062 - val_loss: 606740.1250\n",
      "Epoch 3/29\n",
      "200/200 [==============================] - 47s 237ms/step - loss: 583190.3750 - val_loss: 608305.8750\n",
      "Epoch 4/29\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 453832.6250 - val_loss: 610380.5000\n",
      "Epoch 5/29\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 358588.1562 - val_loss: 614474.5000\n",
      "Epoch 6/29\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 529148.8125 - val_loss: 641545.8750\n",
      "Epoch 7/29\n",
      "200/200 [==============================] - 49s 247ms/step - loss: 485231.4062 - val_loss: 628266.5000\n",
      "Epoch 8/29\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 631927.9375 - val_loss: 678875.5000\n",
      "Epoch 9/29\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 839400.3750 - val_loss: 691134.9375\n",
      "Epoch 10/29\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 750857.4375 - val_loss: 674130.8125\n",
      "Epoch 11/29\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 541714.0000 - val_loss: 674576.1875\n",
      "Epoch 12/29\n",
      "200/200 [==============================] - 47s 233ms/step - loss: 633156.0625 - val_loss: 685097.8125\n",
      "Epoch 13/29\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 531175.0000 - val_loss: 663888.0625\n",
      "Epoch 14/29\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 755886.3750 - val_loss: 670932.0625\n",
      "Epoch 15/29\n",
      "200/200 [==============================] - 7671s 38s/step - loss: 454327.1562 - val_loss: 668567.7500\n",
      "Epoch 16/29\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 401269.0312 - val_loss: 666761.7500\n",
      "Epoch 17/29\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 461688.9062 - val_loss: 749149.3125\n",
      "Epoch 18/29\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 422644.1562 - val_loss: 703328.0625\n",
      "Epoch 19/29\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 435419.8125 - val_loss: 752902.3750\n",
      "Epoch 20/29\n",
      "200/200 [==============================] - 30s 150ms/step - loss: 493155.8438 - val_loss: 701379.0625\n",
      "Epoch 21/29\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 406473.5938 - val_loss: 648108.9375\n",
      "Epoch 22/29\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 456757.8438 - val_loss: 640000.6250\n",
      "Epoch 23/29\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 493756.8125 - val_loss: 624226.0625\n",
      "Epoch 24/29\n",
      "200/200 [==============================] - 47s 236ms/step - loss: 426625.2812 - val_loss: 627978.1250\n",
      "Epoch 25/29\n",
      "200/200 [==============================] - 47s 237ms/step - loss: 528311.2500 - val_loss: 663757.3750\n",
      "Epoch 26/29\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 794146.5625 - val_loss: 649230.5000\n",
      "Epoch 27/29\n",
      "200/200 [==============================] - 49s 246ms/step - loss: 377145.2500 - val_loss: 684763.8125\n",
      "Epoch 28/29\n",
      "200/200 [==============================] - 52s 261ms/step - loss: 710012.6250 - val_loss: 731575.5000\n",
      "Epoch 29/29\n",
      "200/200 [==============================] - 51s 255ms/step - loss: 406747.0000 - val_loss: 749608.0000\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 433926.3750 - val_loss: 774850.9375\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 49s 246ms/step - loss: 508437.3750 - val_loss: 777493.0625\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 586899.1250 - val_loss: 774624.2500\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 451947.5938 - val_loss: 776580.6250\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 359968.2500 - val_loss: 783166.3125\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 527780.7500 - val_loss: 810709.2500\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 482800.0000 - val_loss: 798848.7500\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 627469.9375 - val_loss: 864792.2500\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 838461.4375 - val_loss: 860259.0625\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 48s 241ms/step - loss: 740935.9375 - val_loss: 873675.2500\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 49s 246ms/step - loss: 541782.8750 - val_loss: 830043.5000\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 47s 236ms/step - loss: 633180.0000 - val_loss: 872943.5000\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 527901.6875 - val_loss: 842829.5000\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 48s 238ms/step - loss: 759186.7500 - val_loss: 814717.5000\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 47s 237ms/step - loss: 448704.5625 - val_loss: 842739.6250\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 48s 240ms/step - loss: 403157.4375 - val_loss: 858974.2500\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 463666.3125 - val_loss: 847983.8125\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 47s 236ms/step - loss: 426228.7500 - val_loss: 854615.2500\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 47s 237ms/step - loss: 431454.2500 - val_loss: 875101.3750\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 493594.5938 - val_loss: 885897.2500\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 50s 249ms/step - loss: 407083.4375 - val_loss: 844805.8750\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 49s 247ms/step - loss: 455775.4688 - val_loss: 918796.6875\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 48s 239ms/step - loss: 488119.9688 - val_loss: 812588.6875\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 49s 245ms/step - loss: 427028.5312 - val_loss: 805259.2500\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 528638.2500 - val_loss: 814576.5000\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 1743s 9s/step - loss: 797244.8125 - val_loss: 813338.8750\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 373012.8125 - val_loss: 854681.5000\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 708668.0000 - val_loss: 1002887.4375\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 405954.5938 - val_loss: 939238.6250\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 18s 90ms/step - loss: 3231169.5000 - val_loss: 912058.1875\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 57s 286ms/step - loss: 433949.9062 - val_loss: 527931.4375\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 52s 261ms/step - loss: 506910.1562 - val_loss: 528616.8125\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 585737.1875 - val_loss: 531737.3750\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 51s 255ms/step - loss: 453804.0938 - val_loss: 537654.2500\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 372191.2500 - val_loss: 556993.0625\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 52s 260ms/step - loss: 511176.9688 - val_loss: 594195.8750\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 485547.5312 - val_loss: 561148.1875\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 625465.0625 - val_loss: 616239.5000\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 51s 253ms/step - loss: 841579.6875 - val_loss: 559174.3125\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 52s 260ms/step - loss: 744298.3125 - val_loss: 554959.6250\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 52s 261ms/step - loss: 538627.5000 - val_loss: 555679.1250\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 50s 252ms/step - loss: 636604.0000 - val_loss: 559553.6250\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 528036.2500 - val_loss: 552101.1250\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 51s 254ms/step - loss: 759507.0625 - val_loss: 532874.0625\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 48s 242ms/step - loss: 450472.6250 - val_loss: 563542.3750\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 401830.6250 - val_loss: 577213.5625\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 460894.1562 - val_loss: 624039.1250\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 51s 256ms/step - loss: 423553.9688 - val_loss: 611107.9375\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 432610.5938 - val_loss: 678863.5625\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 51s 254ms/step - loss: 490157.2812 - val_loss: 596712.1875\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 50s 251ms/step - loss: 406756.8438 - val_loss: 613336.6875\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 454794.3125 - val_loss: 605343.5000\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 52s 260ms/step - loss: 490761.0000 - val_loss: 551394.6250\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 50s 248ms/step - loss: 426172.0312 - val_loss: 540960.7500\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 528262.6875 - val_loss: 543573.6250\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 51s 256ms/step - loss: 789164.2500 - val_loss: 582447.1875\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 50s 250ms/step - loss: 368835.8438 - val_loss: 567246.9375\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 704077.4375 - val_loss: 657924.3750\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 49s 244ms/step - loss: 406905.1562 - val_loss: 665496.0000\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 33s 166ms/step - loss: 3228470.0000 - val_loss: 621355.6250\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 432731.0312 - val_loss: 1222013.8750\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 510782.0938 - val_loss: 1225583.5000\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 585489.6250 - val_loss: 1213208.5000\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 452852.1562 - val_loss: 1210071.1250\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 357830.1250 - val_loss: 1210345.0000\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 529907.7500 - val_loss: 1225696.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 484962.4062 - val_loss: 1219148.0000\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 630273.0000 - val_loss: 1338708.2500\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 815565.9375 - val_loss: 1305619.3750\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 764479.9375 - val_loss: 1297882.2500\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 541802.2500 - val_loss: 1237038.5000\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 632443.8125 - val_loss: 1274655.6250\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 529520.9375 - val_loss: 1236529.5000\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 761013.3750 - val_loss: 1241995.5000\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 446054.3750 - val_loss: 1285465.2500\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 401975.0000 - val_loss: 1227939.5000\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 459831.5625 - val_loss: 1309096.7500\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 424415.1562 - val_loss: 1286044.1250\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 433369.6250 - val_loss: 1374217.2500\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 15s 76ms/step - loss: 493746.7188 - val_loss: 1260182.6250\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 407843.8438 - val_loss: 1271034.0000\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 15s 77ms/step - loss: 456435.2500 - val_loss: 1322734.6250\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 490875.6875 - val_loss: 1254201.2500\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 426792.2812 - val_loss: 1260564.3750\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 528952.7500 - val_loss: 1262784.0000\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 795567.5000 - val_loss: 1274687.0000\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 371392.1250 - val_loss: 1368545.5000\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 15s 76ms/step - loss: 708793.8125 - val_loss: 1368183.1250\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 405985.9688 - val_loss: 1544970.7500\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 15s 77ms/step - loss: 3219243.0000 - val_loss: 1304515.6250\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 434484.4375 - val_loss: 573046.1250\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 507944.6875 - val_loss: 574481.7500\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 587822.5000 - val_loss: 580186.1875\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 451028.9688 - val_loss: 575766.6875\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 358844.5938 - val_loss: 580190.4375\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 525906.5625 - val_loss: 618265.8125\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 485201.3125 - val_loss: 604197.8125\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 628013.9375 - val_loss: 601425.2500\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 843366.8125 - val_loss: 616939.2500\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 745374.3125 - val_loss: 659983.2500\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 541625.8125 - val_loss: 636783.3125\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 18s 89ms/step - loss: 633948.8125 - val_loss: 608985.8750\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 528718.6250 - val_loss: 668695.1875\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 759316.0625 - val_loss: 612081.3750\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 452193.7188 - val_loss: 637649.6250\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 403826.7188 - val_loss: 616973.6250\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 470424.3125 - val_loss: 639188.9375\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 423892.1562 - val_loss: 665634.9375\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 435276.1562 - val_loss: 697091.3750\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 494342.1250 - val_loss: 724299.1250\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 405737.9688 - val_loss: 781714.7500\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 455414.5312 - val_loss: 733325.1250\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 489816.7500 - val_loss: 599356.5000\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 429089.6875 - val_loss: 599327.5000\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 530781.8750 - val_loss: 670827.0625\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 797739.7500 - val_loss: 698120.8750\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 371412.5625 - val_loss: 651336.1875\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 710488.5625 - val_loss: 760461.8125\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 405932.1250 - val_loss: 693756.6250\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 3222027.7500 - val_loss: 618629.7500\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/30\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 434338.6875 - val_loss: 590676.3125\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 505109.9062 - val_loss: 593268.5625\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 591218.8125 - val_loss: 603334.0625\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 450950.1250 - val_loss: 591604.9375\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 360194.4375 - val_loss: 596721.5000\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 527611.7500 - val_loss: 640217.1250\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 484113.8438 - val_loss: 605931.8125\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 625313.3125 - val_loss: 646102.6250\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 839263.9375 - val_loss: 646364.1875\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 743162.7500 - val_loss: 663507.8125\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 538283.5625 - val_loss: 659540.1250\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 636804.3750 - val_loss: 660532.1250\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 527254.2500 - val_loss: 675243.8125\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 760814.2500 - val_loss: 662582.6250\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 448046.8438 - val_loss: 676688.4375\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 398822.6875 - val_loss: 651301.5625\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 457252.0938 - val_loss: 711071.4375\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 420195.3125 - val_loss: 740992.0625\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 433608.6250 - val_loss: 718435.0000\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 492475.4688 - val_loss: 742633.4375\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 409433.8750 - val_loss: 701895.5625\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 456806.9062 - val_loss: 642951.0625\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 489407.3125 - val_loss: 603687.5625\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 427211.0938 - val_loss: 606297.0000\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 529246.8125 - val_loss: 636304.8750\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 800521.3750 - val_loss: 679543.1875\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 370375.1250 - val_loss: 778494.5000\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 708617.7500 - val_loss: 741744.5625\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 404458.5312 - val_loss: 702852.3125\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 3229025.2500 - val_loss: 639757.5000\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 434635.4062 - val_loss: 590622.6250\n",
      "Epoch 2/29\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 506719.5312 - val_loss: 591980.5000\n",
      "Epoch 3/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 586581.3750 - val_loss: 591066.3750\n",
      "Epoch 4/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 453402.4688 - val_loss: 592617.5625\n",
      "Epoch 5/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 358940.0000 - val_loss: 601991.3750\n",
      "Epoch 6/29\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 527615.5000 - val_loss: 627094.1250\n",
      "Epoch 7/29\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 484149.0000 - val_loss: 601254.9375\n",
      "Epoch 8/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 631466.6875 - val_loss: 656029.5625\n",
      "Epoch 9/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 815118.1875 - val_loss: 642072.5625\n",
      "Epoch 10/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 768771.5000 - val_loss: 618396.7500\n",
      "Epoch 11/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 542164.1875 - val_loss: 617716.1875\n",
      "Epoch 12/29\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 629341.8750 - val_loss: 636956.8125\n",
      "Epoch 13/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 533375.9375 - val_loss: 613205.9375\n",
      "Epoch 14/29\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 761999.4375 - val_loss: 610743.5625\n",
      "Epoch 15/29\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 449869.5312 - val_loss: 614972.0000\n",
      "Epoch 16/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 399101.8750 - val_loss: 674898.2500\n",
      "Epoch 17/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 464077.8438 - val_loss: 674529.5625\n",
      "Epoch 18/29\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 423766.2812 - val_loss: 627288.8125\n",
      "Epoch 19/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 436667.3750 - val_loss: 647456.1250\n",
      "Epoch 20/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 493115.0000 - val_loss: 609025.0625\n",
      "Epoch 21/29\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 407600.2812 - val_loss: 598326.5625\n",
      "Epoch 22/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 459220.6250 - val_loss: 617354.8750\n",
      "Epoch 23/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 492111.2500 - val_loss: 600639.9375\n",
      "Epoch 24/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 427685.5938 - val_loss: 601716.6875\n",
      "Epoch 25/29\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 530422.5000 - val_loss: 613044.1875\n",
      "Epoch 26/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 794019.1250 - val_loss: 622137.1875\n",
      "Epoch 27/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 374369.8438 - val_loss: 623956.6875\n",
      "Epoch 28/29\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 678950.5000 - val_loss: 758068.8125\n",
      "Epoch 29/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 437877.9062 - val_loss: 906101.9375\n",
      "la fonction objective de l'algorithme de firefly va avoir une luciole en paramère etretourne le taux de perte de modèle basé sur les paramètre de cette luciole\n",
      "Epoch 1/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 435019.3125 - val_loss: 573631.6250\n",
      "Epoch 2/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 507376.3750 - val_loss: 572854.5000\n",
      "Epoch 3/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 585470.6250 - val_loss: 577293.8125\n",
      "Epoch 4/29\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 452573.1875 - val_loss: 577451.1250\n",
      "Epoch 5/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 371959.6250 - val_loss: 597528.6875\n",
      "Epoch 6/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 510167.9688 - val_loss: 612058.1875\n",
      "Epoch 7/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 485648.5625 - val_loss: 624149.7500\n",
      "Epoch 8/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 604069.8125 - val_loss: 627932.7500\n",
      "Epoch 9/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 863480.0000 - val_loss: 603323.6250\n",
      "Epoch 10/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 745975.3750 - val_loss: 610525.3125\n",
      "Epoch 11/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 541570.5625 - val_loss: 588252.0625\n",
      "Epoch 12/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 632407.6250 - val_loss: 590586.6250\n",
      "Epoch 13/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 530109.3750 - val_loss: 578643.9375\n",
      "Epoch 14/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 759570.3750 - val_loss: 575720.3750\n",
      "Epoch 15/29\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 445710.6875 - val_loss: 714667.8125\n",
      "Epoch 16/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 400659.7500 - val_loss: 705867.9375\n",
      "Epoch 17/29\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 465598.4062 - val_loss: 662371.9375\n",
      "Epoch 18/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 424082.1875 - val_loss: 664122.5000\n",
      "Epoch 19/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 432019.4375 - val_loss: 671619.3750\n",
      "Epoch 20/29\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 494554.5625 - val_loss: 689872.8750\n",
      "Epoch 21/29\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 404860.6250 - val_loss: 676883.6250\n",
      "Epoch 22/29\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 446391.6875 - val_loss: 661805.0625\n",
      "Epoch 23/29\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 500433.6875 - val_loss: 653484.7500\n",
      "Epoch 24/29\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 426488.5312 - val_loss: 623306.5625\n",
      "Epoch 25/29\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 527292.2500 - val_loss: 702424.8750\n",
      "Epoch 26/29\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 791539.5000 - val_loss: 720524.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/29\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 374373.0000 - val_loss: 688979.4375\n",
      "Epoch 28/29\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 708285.2500 - val_loss: 759768.6250\n",
      "Epoch 29/29\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 406820.6875 - val_loss: 713347.9375\n"
     ]
    }
   ],
   "source": [
    "print(\"lancement algorithme de firefly..........\")\n",
    "print(\"l'algorithme peut prendre un peu de temps merci de patienter\")\n",
    "optimizer=FireflyOptimizer(Lstm_firefly)\n",
    "brightness,position = optimizer.run_firefly(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'alghoritme a bien terminé et trouvé la bonne ruesultat\n",
      "**********************************best parametre :**********************************\n",
      "796.4490197497768 [ 6 36]\n"
     ]
    }
   ],
   "source": [
    "print(\"l'alghoritme a bien terminé et trouvé la bonne ruesultat\")\n",
    "print(\"**********************************best parametre :**********************************\")\n",
    "print(brightness,position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiiiiiiiiiiiiiiiiiiiiiiinnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\n",
      "lancement d'entrainnement de modèle terminale  en utilisant les meilleur hyperparamètre\n",
      "..................\n",
      "on commmence..................\n",
      "Epoch 1/36\n",
      "200/200 [==============================] - 21s 107ms/step - loss: 427825.0938 - val_loss: 275384.8438\n",
      "Epoch 2/36\n",
      "200/200 [==============================] - 24s 122ms/step - loss: 514515.7188 - val_loss: 276025.5000\n",
      "Epoch 3/36\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 586954.8750 - val_loss: 278298.7812\n",
      "Epoch 4/36\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 451186.1250 - val_loss: 270535.6562\n",
      "Epoch 5/36\n",
      "200/200 [==============================] - 18s 92ms/step - loss: 358373.4375 - val_loss: 267063.7812\n",
      "Epoch 6/36\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 523298.0312 - val_loss: 330794.1562\n",
      "Epoch 7/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 484892.5312 - val_loss: 280707.5938\n",
      "Epoch 8/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 629251.8750 - val_loss: 298495.5625\n",
      "Epoch 9/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 844394.2500 - val_loss: 307106.6562\n",
      "Epoch 10/36\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 748579.8125 - val_loss: 290646.2188\n",
      "Epoch 11/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 545596.5000 - val_loss: 277324.5938\n",
      "Epoch 12/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 634685.5000 - val_loss: 321907.9688\n",
      "Epoch 13/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 533187.2500 - val_loss: 311401.7812\n",
      "Epoch 14/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 755204.5625 - val_loss: 317835.3438\n",
      "Epoch 15/36\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 455287.0312 - val_loss: 410095.3438\n",
      "Epoch 16/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 403795.1875 - val_loss: 342030.9062\n",
      "Epoch 17/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 469503.8125 - val_loss: 348555.4062\n",
      "Epoch 18/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 423455.0312 - val_loss: 312754.9062\n",
      "Epoch 19/36\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 437661.2500 - val_loss: 325196.9062\n",
      "Epoch 20/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 492833.9062 - val_loss: 346282.9688\n",
      "Epoch 21/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 409405.0312 - val_loss: 364988.7812\n",
      "Epoch 22/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 458912.9062 - val_loss: 468767.1562\n",
      "Epoch 23/36\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 493874.5312 - val_loss: 320188.0000\n",
      "Epoch 24/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 429785.9688 - val_loss: 332628.2812\n",
      "Epoch 25/36\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 530928.0000 - val_loss: 340577.1250\n",
      "Epoch 26/36\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 801738.6250 - val_loss: 345664.5312\n",
      "Epoch 27/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 376948.5312 - val_loss: 328315.0938\n",
      "Epoch 28/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 684110.0000 - val_loss: 338206.2188\n",
      "Epoch 29/36\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 440182.9688 - val_loss: 407092.0000\n",
      "Epoch 30/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 3219862.0000 - val_loss: 387726.0312\n",
      "Epoch 31/36\n",
      "200/200 [==============================] - 22s 108ms/step - loss: 514645.9688 - val_loss: 283449.4375\n",
      "Epoch 32/36\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 1351913.6250 - val_loss: 289233.0000\n",
      "Epoch 33/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 615868.6250 - val_loss: 292841.8125\n",
      "Epoch 34/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 746493.6250 - val_loss: 282338.4688\n",
      "Epoch 35/36\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 796162.0000 - val_loss: 291018.0000\n",
      "Epoch 36/36\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 775079.6250 - val_loss: 285406.4062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "796.1949502615203"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fiiiiiiiiiiiiiiiiiiiiiiinnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\")\n",
    "print(\"lancement d'entrainnement de modèle terminale  en utilisant les meilleur hyperparamètre\")\n",
    "print(\"..................\")\n",
    "print(\"on commmence..................\")\n",
    "Lstm_optm(position[1], position[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 434665.8438 - val_loss: 275927.2812\n",
      "Epoch 2/150\n",
      "200/200 [==============================] - 34s 172ms/step - loss: 508440.0938 - val_loss: 274443.4688\n",
      "Epoch 3/150\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 585669.7500 - val_loss: 272572.4688\n",
      "Epoch 4/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 454159.3125 - val_loss: 268529.7500\n",
      "Epoch 5/150\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 373458.8438 - val_loss: 274722.3438\n",
      "Epoch 6/150\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 513404.1562 - val_loss: 298050.0312\n",
      "Epoch 7/150\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 485237.9062 - val_loss: 272424.0312\n",
      "Epoch 8/150\n",
      "200/200 [==============================] - 34s 169ms/step - loss: 632445.5000 - val_loss: 358476.4062\n",
      "Epoch 9/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 819425.8125 - val_loss: 278097.7188\n",
      "Epoch 10/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 766428.2500 - val_loss: 280687.1250\n",
      "Epoch 11/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 539591.6250 - val_loss: 301331.2812\n",
      "Epoch 12/150\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 633424.8125 - val_loss: 342393.2500\n",
      "Epoch 13/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 530103.5000 - val_loss: 298002.3125\n",
      "Epoch 14/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 761124.8750 - val_loss: 300489.8438\n",
      "Epoch 15/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 449283.0312 - val_loss: 343010.4062\n",
      "Epoch 16/150\n",
      "200/200 [==============================] - 34s 168ms/step - loss: 402212.3750 - val_loss: 317236.9062\n",
      "Epoch 17/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 462016.3750 - val_loss: 331849.2188\n",
      "Epoch 18/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 425225.1875 - val_loss: 315295.3750\n",
      "Epoch 19/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 435619.4375 - val_loss: 303546.0312\n",
      "Epoch 20/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 492707.0000 - val_loss: 320113.4688\n",
      "Epoch 21/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 405932.7500 - val_loss: 317418.7812\n",
      "Epoch 22/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 459062.6250 - val_loss: 371418.9062\n",
      "Epoch 23/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 491222.4688 - val_loss: 303233.2812\n",
      "Epoch 24/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 428376.9062 - val_loss: 306165.2812\n",
      "Epoch 25/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 530182.8750 - val_loss: 323113.0000\n",
      "Epoch 26/150\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 793448.0000 - val_loss: 353405.8750\n",
      "Epoch 27/150\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 378132.5625 - val_loss: 288900.2812\n",
      "Epoch 28/150\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 713156.5625 - val_loss: 308803.6250\n",
      "Epoch 29/150\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 406764.2812 - val_loss: 318078.1562\n",
      "Epoch 30/150\n",
      "200/200 [==============================] - 34s 169ms/step - loss: 3218832.0000 - val_loss: 296992.9688\n",
      "Epoch 31/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 526616.2500 - val_loss: 287157.4062\n",
      "Epoch 32/150\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 1335352.7500 - val_loss: 289536.8438\n",
      "Epoch 33/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 619086.1250 - val_loss: 271940.4062\n",
      "Epoch 34/150\n",
      "200/200 [==============================] - 34s 171ms/step - loss: 741890.0000 - val_loss: 275539.7812\n",
      "Epoch 35/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 801998.6250 - val_loss: 292339.5000\n",
      "Epoch 36/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 767532.0625 - val_loss: 295225.5938\n",
      "Epoch 37/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 759398.2500 - val_loss: 277869.9062\n",
      "Epoch 38/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 1106324.7500 - val_loss: 267867.3438\n",
      "Epoch 39/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 678800.7500 - val_loss: 260584.0000\n",
      "Epoch 40/150\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 510698.7188 - val_loss: 259856.6406\n",
      "Epoch 41/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 829495.6875 - val_loss: 270236.2188\n",
      "Epoch 42/150\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 606378.8750 - val_loss: 272507.9688\n",
      "Epoch 43/150\n",
      "200/200 [==============================] - 33s 167ms/step - loss: 1219422.8750 - val_loss: 282223.4062\n",
      "Epoch 44/150\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 3670544.7500 - val_loss: 260129.6719\n",
      "Epoch 45/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 43489860.0000 - val_loss: 286579.7188\n",
      "Epoch 46/150\n",
      "200/200 [==============================] - 35s 175ms/step - loss: 3441233.2500 - val_loss: 284482.6562\n",
      "Epoch 47/150\n",
      "200/200 [==============================] - 33s 167ms/step - loss: 1047215.0625 - val_loss: 279894.2188\n",
      "Epoch 48/150\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 693843.5000 - val_loss: 273861.0312\n",
      "Epoch 49/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 570598.2500 - val_loss: 255359.8750\n",
      "Epoch 50/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 655013.5000 - val_loss: 262787.1562\n",
      "Epoch 51/150\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 478054.7812 - val_loss: 255483.6719\n",
      "Epoch 52/150\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 415031.7500 - val_loss: 270358.9375\n",
      "Epoch 53/150\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 485455.9688 - val_loss: 289832.4688\n",
      "Epoch 54/150\n",
      "200/200 [==============================] - 33s 167ms/step - loss: 559250.6250 - val_loss: 298079.5312\n",
      "Epoch 55/150\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 436473.1562 - val_loss: 262663.3438\n",
      "Epoch 56/150\n",
      "200/200 [==============================] - 31s 157ms/step - loss: 340260.8125 - val_loss: 267975.5938\n",
      "Epoch 57/150\n",
      "200/200 [==============================] - 31s 156ms/step - loss: 477427.3750 - val_loss: 279588.3438\n",
      "Epoch 58/150\n",
      "200/200 [==============================] - 31s 156ms/step - loss: 474590.9062 - val_loss: 270908.9688\n",
      "Epoch 59/150\n",
      "200/200 [==============================] - 31s 156ms/step - loss: 593970.2500 - val_loss: 271956.2188\n",
      "Epoch 60/150\n",
      "200/200 [==============================] - 31s 156ms/step - loss: 811756.8750 - val_loss: 259136.5625\n",
      "Epoch 61/150\n",
      "200/200 [==============================] - 34s 172ms/step - loss: 724889.1250 - val_loss: 262591.4062\n",
      "Epoch 62/150\n",
      "200/200 [==============================] - 54s 270ms/step - loss: 669069.1250 - val_loss: 261660.0469\n",
      "Epoch 63/150\n",
      "200/200 [==============================] - 57s 287ms/step - loss: 481661.9688 - val_loss: 305433.8125\n",
      "Epoch 64/150\n",
      "200/200 [==============================] - 58s 289ms/step - loss: 615385.9375 - val_loss: 263563.5000\n",
      "Epoch 65/150\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 717607.0625 - val_loss: 273868.8438\n",
      "Epoch 66/150\n",
      "200/200 [==============================] - 57s 286ms/step - loss: 340520.5625 - val_loss: 277782.7188\n",
      "Epoch 67/150\n",
      "200/200 [==============================] - 56s 279ms/step - loss: 379765.6875 - val_loss: 265952.9375\n",
      "Epoch 68/150\n",
      "200/200 [==============================] - 57s 285ms/step - loss: 429643.6875 - val_loss: 267247.7812\n",
      "Epoch 69/150\n",
      "200/200 [==============================] - 55s 273ms/step - loss: 401414.1250 - val_loss: 274102.0312\n",
      "Epoch 70/150\n",
      "200/200 [==============================] - 57s 286ms/step - loss: 414568.2500 - val_loss: 271838.5938\n",
      "Epoch 71/150\n",
      "200/200 [==============================] - 56s 279ms/step - loss: 492949.4062 - val_loss: 257777.6406\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 61s 307ms/step - loss: 382833.0000 - val_loss: 270044.4062\n",
      "Epoch 73/150\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 451444.9688 - val_loss: 292295.2812\n",
      "Epoch 74/150\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 480968.3125 - val_loss: 288799.3438\n",
      "Epoch 75/150\n",
      "200/200 [==============================] - 49s 243ms/step - loss: 413426.5938 - val_loss: 292762.1562\n",
      "Epoch 76/150\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 514176.1250 - val_loss: 274643.4688\n",
      "Epoch 77/150\n",
      "200/200 [==============================] - 22s 108ms/step - loss: 798322.5000 - val_loss: 282151.2188\n",
      "Epoch 78/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 347619.3125 - val_loss: 271854.2812\n",
      "Epoch 79/150\n",
      "200/200 [==============================] - 23s 113ms/step - loss: 689351.5000 - val_loss: 285344.6250\n",
      "Epoch 80/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 1887663.5000 - val_loss: 268012.8750\n",
      "Epoch 81/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 1742266.5000 - val_loss: 269965.7812\n",
      "Epoch 82/150\n",
      "200/200 [==============================] - 23s 114ms/step - loss: 499152.4688 - val_loss: 276160.3750\n",
      "Epoch 83/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 1327377.5000 - val_loss: 276105.0312\n",
      "Epoch 84/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 612241.3750 - val_loss: 272822.6562\n",
      "Epoch 85/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 737325.8125 - val_loss: 287777.9688\n",
      "Epoch 86/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 810835.1250 - val_loss: 300989.9062\n",
      "Epoch 87/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 784906.0625 - val_loss: 277166.9062\n",
      "Epoch 88/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 695650.0000 - val_loss: 283686.7188\n",
      "Epoch 89/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 1089825.6250 - val_loss: 270710.2812\n",
      "Epoch 90/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 718520.8125 - val_loss: 271618.9062\n",
      "Epoch 91/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 458754.4688 - val_loss: 261713.2031\n",
      "Epoch 92/150\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 819329.4375 - val_loss: 267855.9062\n",
      "Epoch 93/150\n",
      "200/200 [==============================] - 22s 108ms/step - loss: 598973.0625 - val_loss: 278503.1250\n",
      "Epoch 94/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 1218784.3750 - val_loss: 278323.2188\n",
      "Epoch 95/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 3665553.5000 - val_loss: 267050.1562\n",
      "Epoch 96/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 43479848.0000 - val_loss: 263449.2500\n",
      "Epoch 97/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 3487016.7500 - val_loss: 278568.2500\n",
      "Epoch 98/150\n",
      "200/200 [==============================] - 23s 113ms/step - loss: 1003838.2500 - val_loss: 271258.3438\n",
      "Epoch 99/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 682349.4375 - val_loss: 269338.5938\n",
      "Epoch 100/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 606424.0625 - val_loss: 261129.3906\n",
      "Epoch 101/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 621453.9375 - val_loss: 274820.7188\n",
      "Epoch 102/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 469936.9688 - val_loss: 270492.6562\n",
      "Epoch 103/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 408755.4062 - val_loss: 270963.8125\n",
      "Epoch 104/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 479016.0000 - val_loss: 299240.0312\n",
      "Epoch 105/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 541990.5625 - val_loss: 362062.3438\n",
      "Epoch 106/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 426649.9062 - val_loss: 281767.7188\n",
      "Epoch 107/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 353947.0000 - val_loss: 315765.5938\n",
      "Epoch 108/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 438100.5625 - val_loss: 287558.4688\n",
      "Epoch 109/150\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 459537.9688 - val_loss: 277232.0000\n",
      "Epoch 110/150\n",
      "200/200 [==============================] - 23s 114ms/step - loss: 591482.8125 - val_loss: 281655.6875\n",
      "Epoch 111/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 789164.9375 - val_loss: 276465.9062\n",
      "Epoch 112/150\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 728170.5625 - val_loss: 286910.6562\n",
      "Epoch 113/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 649432.5000 - val_loss: 280227.5625\n",
      "Epoch 114/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 487188.3125 - val_loss: 299785.2500\n",
      "Epoch 115/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 584688.6875 - val_loss: 282959.2500\n",
      "Epoch 116/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 705460.6250 - val_loss: 295494.6562\n",
      "Epoch 117/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 325781.0000 - val_loss: 302022.4688\n",
      "Epoch 118/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 369523.2500 - val_loss: 317845.6250\n",
      "Epoch 119/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 416904.2812 - val_loss: 289353.2500\n",
      "Epoch 120/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 394570.0000 - val_loss: 318535.9688\n",
      "Epoch 121/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 403988.4062 - val_loss: 311000.9062\n",
      "Epoch 122/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 479623.5938 - val_loss: 274830.3125\n",
      "Epoch 123/150\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 388287.0312 - val_loss: 281192.3750\n",
      "Epoch 124/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 430837.1250 - val_loss: 291470.2500\n",
      "Epoch 125/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 476789.6875 - val_loss: 271903.3438\n",
      "Epoch 126/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 410155.3125 - val_loss: 269245.5938\n",
      "Epoch 127/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 490944.6875 - val_loss: 274846.2500\n",
      "Epoch 128/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 786260.6250 - val_loss: 277052.5000\n",
      "Epoch 129/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 341396.6875 - val_loss: 284470.5938\n",
      "Epoch 130/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 685161.2500 - val_loss: 287531.2188\n",
      "Epoch 131/150\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 1868492.5000 - val_loss: 283333.1562\n",
      "Epoch 132/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 1737164.6250 - val_loss: 270701.5938\n",
      "Epoch 133/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 613958.1875 - val_loss: 277553.5938\n",
      "Epoch 134/150\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 1213850.3750 - val_loss: 267147.9688\n",
      "Epoch 135/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 582018.7500 - val_loss: 270379.0000\n",
      "Epoch 136/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 861586.3125 - val_loss: 296213.2500\n",
      "Epoch 137/150\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 702487.6875 - val_loss: 284226.4688\n",
      "Epoch 138/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 757714.8125 - val_loss: 274280.4062\n",
      "Epoch 139/150\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 736267.6875 - val_loss: 280236.8750\n",
      "Epoch 140/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 1024206.6250 - val_loss: 273969.6250\n",
      "Epoch 141/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 711313.7500 - val_loss: 266754.3438\n",
      "Epoch 142/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 450443.0000 - val_loss: 268908.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 834236.6250 - val_loss: 281065.7188\n",
      "Epoch 144/150\n",
      "200/200 [==============================] - 22s 108ms/step - loss: 571699.8125 - val_loss: 281130.1562\n",
      "Epoch 145/150\n",
      "200/200 [==============================] - 23s 113ms/step - loss: 1226738.1250 - val_loss: 300595.7500\n",
      "Epoch 146/150\n",
      "200/200 [==============================] - 22s 108ms/step - loss: 4237394.5000 - val_loss: 273721.9062\n",
      "Epoch 147/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 42872816.0000 - val_loss: 280356.6250\n",
      "Epoch 148/150\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 3493347.5000 - val_loss: 290365.4688\n",
      "Epoch 149/150\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 978682.1875 - val_loss: 268095.2812\n",
      "Epoch 150/150\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 677686.2500 - val_loss: 270376.6562\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "dropout = 0.0\n",
    "simple_lstm_model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.LSTM(\n",
    "    128, input_shape=X_train_w.shape[-2:], dropout=dropout),\n",
    "tf.keras.layers.Dense(128),\n",
    "tf.keras.layers.Dense(128),\n",
    "tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "simple_lstm_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "\n",
    "\n",
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 150\n",
    "\n",
    "model_history = simple_lstm_model.fit(train_data, epochs=EPOCHS,\n",
    "                                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                      validation_data=val_data, validation_steps=6)  # ,callbacks=[tensorboard_callback]) #Uncomment this line for tensorboard support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lstm_model.save('C:/Users/lenovo/PycharmProjects/forcoasting/my_model_supliers.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
